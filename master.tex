\documentclass[%
thesis=student,% bachlor's or master's thesis
coverpage=false,% do not print an extra cover page
titlepage=false,% do not print an extra title page
headmarks=true, % headmarks can be switched on or off
english,% or `german`
font=libertine, % use `libertine` font; alternatives: `helvet` / `palatino` / `times`
math=newpxtx, % math font `newpxtx`; alternatives: `ams`, `pxtx`
BCOR=5mm,% binding correction - adapt accordingly
coverBCOR=11mm% binding correction for the cover - adapt accordingly
]{tumbook}

\makeatletter %redefine some labels from the TUM template
\provideName{\@tum@examiner@}{Supervisor}{Themensteller} % or `Themenstellerin`
\provideName{\@tum@supervisor@}{Advisors}{Betreuer} % or `Advisor` / `Betreuerin`
\makeatother

\usepackage{booktabs}% for more beautiful tables
\usepackage{xcolor}

\usepackage{diffcoeff}
\usepackage{amsmath}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{theorem}[definition]{Satz}
\usepackage{fvextra} 

%Literatur
\usepackage[%
    backend=bibtex, %, or `biber` on more up-to-date systems
    sortcites, % sort automatically
    sorting=nty, % sort order
    safeinputenc, % solves problems with unicode-formatted author names etc.
    citestyle=alphabetic, %
    bibstyle=alphabetic, %
    hyperref=true, % provide clickable links
    maxbibnames=3, % shorten author list for more than 3 names
    maxcitenames=3, % use at most 3 names for key
    url=false, % do not print URLs
    doi=false, % do not print DOIs
    giveninits=true,
    ]%
{biblatex}
\addbibresource{Kirill-Thesis.bib}

% HYPERLINKS AND CLEVEREF
\usepackage{cleveref}% intelligent references

% automatische Anführungszeichen
\usepackage[autostyle=true]{csquotes}


\title{Implicit Context Condensation for Local Software Engineering Agents}
%%%\subtitle{A Comprehensive Study on LLM Context Management}

\author{Kirill Gelvan}

\degree{Master of Science}% or `Bachlor of Science`
\dateSubmitted{30. November 2025}% preferably use some universally recognized date format


\examiner{Prof.\@ Dr.\@ Gjergji Kasneci}% `Themensteller`
\supervisor{Felix Steinbauer\\Igor Slinko}% `Betreuer`


\begin{document}

\frontmatter
\maketitle


\section*{Abstract}
This thesis investigates implicit context condensation for large language model based software engineering agents.
Instead of relying on extended context windows or explicit methods, it compresses long observations into a fixed set of embeddings using an in-context LLM-based encoder attached to a frozen LLM decoder.
The compressor is pretrained on general text with a mix of autoencoding and language modeling objectives and then fine-tuned on question answering, code reconstruction, or agentic trajectories derived from SWE-bench style issues.
Experiments on SQuAD and RepoQA show that the condensed representations can match or even surpass the uncompressed baselines on single-shot tasks, preserving enough detail to reconstruct natural language and source code.
In the agentic setting, context condensation enables trajectories with more steps within a fixed context budget and reduces per-step inference time, demonstrating clear efficiency gains.
However, when the agent is applied to the end-to-end setting of SWE-bench Verified, these benefits do not lead to the higher resolve rate and underperform an untrained baseline.
% Negative results for training-free and low-capacity projection baselines, together with evidence of a bottleneck in parameter-efficient fine-tuning, indicate that effective context condensation for autonomous software engineering will require higher-fidelity compression and stronger optimization than explored here.

\section*{Zusammenfassung}
Diese Arbeit untersucht die implizite Kontextkomprimierung für Software-Engineering-Agenten, die auf Large Language Models (LLMs) basieren. Anstatt sich auf erweiterte Kontextfenster oder explizite Methoden zu stützen, werden lange Observationen hierbei in ein festes Set von Embeddings komprimiert. Dies geschieht mithilfe eines In-Context-Encoders auf LLM-Basis, der an einen statischen („frozen") LLM Decoder gekoppelt ist.
Der Kompressor wird zunächst auf allgemeinen Textdaten vortrainiert (Pretraining), wobei eine Mischung aus Autoencoding- und Sprachmodellierungs-Zielen zum Einsatz kommt. Anschließend erfolgt ein Fine-Tuning auf Question-Answering, Code-Rekonstruktion oder auf Trajektorien, die von Problemen im Stil des SWE-bench abgeleitet sind.
Experimente mit SQuAD und RepoQA zeigen, dass die komprimierten Repräsentationen in Single-Shot-Aufgaben mit unkomprimierten Baselines mithalten oder diese sogar übertreffen können. Dabei bleiben genügend Details erhalten, um sowohl natürliche Sprache als auch Quellcode zu rekonstruieren. Beim Einsatz als Agent ermöglicht die Kontextkomprimierung längere Handlungssequenzen innerhalb eines festen Kontext-Budgets sowie eine reduzierte Inferenzzeit pro Schritt, was deutliche Effizienzgewinne demonstriert.
In der End-to-End-Anwendung auf SWE-bench Verified führen diese Vorteile jedoch nicht zu einer höheren Lösungsrate; das System schneidet hierbei schlechter ab als eine untrainierte Baseline.

\cleardoublepage{}

\hypertarget{toc}{}
\tableofcontents


\mainmatter{}

% Add backlink to Contents in footer next to the page number (only in mainmatter)
\renewcommand*{\pagemark}{{\hyperlink{toc}{$\leftarrow$~}\thepage}}


% ========================================
% CHAPTER FILES
% ========================================
% Each chapter is now in its own file for better organization
\input{chapters/chapter1_introduction}
\input{chapters/chapter2_background}
\input{chapters/chapter3_related_work}
\input{chapters/chapter4_methods}
\input{chapters/chapter5_evaluation}
\input{chapters/chapter6_experiments}
\input{chapters/chapter7_discussion}
\input{chapters/chapter8_limitations}
\input{chapters/chapter9_conclusion}
\input{chapters/appendix}

% ========================================
% BACKMATTER SECTION
% ========================================
% Lists of figures and tables (may be removed if not needed)
\backmatter{}
\listoffigures% may be removed
\listoftables% may be removed

% ========================================
% BIBLIOGRAPHY SECTION
% ========================================
% Add any additional citations that haven't been referenced in the text
\nocite{*} % include all bibliography entries
\printbibliography{} % print bibliography

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: default
%%% TeX-command-extra-options: "-shell-escape"
%%% ispell-local-dictionary: "american"
%%% eval: (setenv "TEXINPUTS" ".//:")
%%% TeX-master: t
%%% End:
