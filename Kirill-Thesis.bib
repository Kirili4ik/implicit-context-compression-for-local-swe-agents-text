
@misc{zhao_position_2025,
	title = {Position {IDs} {Matter}: {An} {Enhanced} {Position} {Layout} for {Efficient} {Context} {Compression} in {Large} {Language} {Models}},
	shorttitle = {Position {IDs} {Matter}},
	url = {http://arxiv.org/abs/2409.14364},
	doi = {10.48550/arXiv.2409.14364},
	abstract = {Using special tokens (e.g., gist, memory, or compressed tokens) to compress context information is a common practice for large language models (LLMs). However, existing approaches often neglect that position encodings inherently induce local inductive biases in models, causing the compression process to ignore holistic contextual dependencies. We propose {\textbackslash}textbf\{Enhanced Position Layout (EPL)\}, a simple yet effective method that improves the context compression capability of LLMs by only adjusting position IDs, the numerical identifiers that specify token positions. EPL minimizes the distance between context tokens and their corresponding special tokens and at the same time maintains the sequence order in position IDs between context tokens, special tokens, and the subsequent tokens. Integrating EPL into our best performing context compression model results in a 1.9 ROUGE-1 F1 improvement on out-of-domain question answering datasets on average. When extended to multimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for vision compression LLMs.},
	urldate = {2025-10-08},
	publisher = {arXiv},
	author = {Zhao, Runsong and Liu, Xin and Liu, Xinyu and Huang, Pengcheng and Xiao, Chunyang and Xiao, Tong and Zhu, Jingbo},
	month = sep,
	year = {2025},
	note = {arXiv:2409.14364 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/Kirill.Gelvan/Zotero/storage/BKEQSJC5/Zhao et al. - 2025 - Position IDs Matter An Enhanced Position Layout for Efficient Context Compression in Large Language.pdf:application/pdf;Snapshot:/Users/Kirill.Gelvan/Zotero/storage/GLUAX6T4/2409.html:text/html},
}

@misc{ge_-context_2024,
	title = {In-context {Autoencoder} for {Context} {Compression} in a {Large} {Language} {Model}},
	url = {http://arxiv.org/abs/2307.06945},
	doi = {10.48550/arXiv.2307.06945},
	abstract = {We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1\% additional parameters, effectively achieves \$4{\textbackslash}times\$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.},
	urldate = {2025-10-10},
	publisher = {arXiv},
	author = {Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
	month = may,
	year = {2024},
	note = {arXiv:2307.06945 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Kirill.Gelvan/Zotero/storage/UE55FTJ9/Ge et al. - 2024 - In-context Autoencoder for Context Compression in a Large Language Model.pdf:application/pdf;Snapshot:/Users/Kirill.Gelvan/Zotero/storage/PSYMBTWP/2307.html:text/html},
}

@misc{beltagy_longformer_2020,
  title         = {Longformer: The Long-Document Transformer},
  author        = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year          = {2020},
  eprint        = {2004.05150},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{zaheer_bigbird_2020,
  title         = {Big Bird: Transformers for Longer Sequences},
  author        = {Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  year          = {2020},
  eprint        = {2007.14062},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{child_sparse_2019,
  title         = {Generating Long Sequences with Sparse Transformers},
  author        = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
  year          = {2019},
  eprint        = {1904.10509},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{katharopoulos_transformers_2020,
  title         = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author        = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Fran{\c c}ois Fleuret},
  year          = {2020},
  eprint        = {2006.16236},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{choromanski_performer_2021,
  title         = {Rethinking Attention with Performers},
  author        = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tam{\'a}s Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy J. Colwell and Adrian Weller},
  year          = {2021},
  eprint        = {2009.14794},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{wang_linformer_2020,
  title         = {Linformer: Self-Attention with Linear Complexity},
  author        = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
  year          = {2020},
  eprint        = {2006.04768},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{lewis_rag_2020,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP}},
  author        = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{"u}ttler and Mike Lewis and Wen-tau Yih and Tim Rockt{"a}schel and Sebastian Riedel and Douwe Kiela},
  year          = {2020},
  eprint        = {2005.11401},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{li_prefix_2021,
  title         = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author        = {Xiang Lisa Li and Percy Liang},
  year          = {2021},
  eprint        = {2101.00190},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{lester_prompt_2021,
  title         = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  author        = {Brian Lester and Rami Al-Rfou and Noah Constant},
  year          = {2021},
  eprint        = {2104.08691},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{zhang_pegasus_2020,
  title         = {PEGASUS: Pre-training with Extracted Gaps Sentences for Abstractive Summarization},
  author        = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
  year          = {2020},
  eprint        = {1912.08777},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{attentionrag_2025,
  title         = {AttentionRAG: Attention-Guided Context Pruning for {RAG}},
  author        = {Anonymous},
  year          = {2025},
  eprint        = {2503.10720},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
}

@misc{block_attention_rag_2024,
  title         = {Block-Attention for Efficient {RAG}},
  author        = {Anonymous},
  year          = {2024},
  eprint        = {2409.15355},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
}

@inproceedings{vaswani_attention_2017,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017}
}

@inproceedings{shaw_relative_2018,
  title     = {Self-Attention with Relative Position Representations},
  author    = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  booktitle = {NAACL-HLT},
  year      = {2018}
}

@misc{su_roformer_2021,
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author        = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  year          = {2021},
  eprint        = {2104.09864},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{hu2021lora,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  year          = {2021},
  eprint        = {2106.09685},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{yao_react_2022,
  title         = {ReAct: Synergizing Reasoning and Acting in Language Models},
  author        = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year          = {2022},
  eprint        = {2210.03629},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{schick_toolformer_2023,
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  author        = {Timo Schick and Jane Dwivedi-Yu and Noah Constant and Te-Yen Wu and Xi Victoria Lin and Sushant Prakash and Urvashi Khandelwal and Ben Athiwaratkun and Kuang-Huei Lee and Ed Chi and Mike Lewis and Colin Raffel and Holger Schwenk and Sebastian Ruder},
  year          = {2023},
  eprint        = {2302.04761},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{nakano_webgpt_2021,
  title         = {WebGPT: Browser-assisted question-answering with human feedback},
  author        = {Reiichiro Nakano and Jacob Pfau and Laria Reynolds and Kyle McDonell and Joar Skalse and Olli Hansson and Owain Evans and William Saunders and Matthew Rahtz and Amanda Askell and John Schulman and Jan Leike},
  year          = {2021},
  eprint        = {2112.09332},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{swesmith,
  title         = {SWE-smith: Agent Setup and Prompt for SWE-bench Verified},
  note          = {Describes the SWE-smith tool protocol and prompt for SWE-bench Verified. Prompt text included in Appendix~\ref{app:swe-smith-prompt}.},
  year          = {2024},
  howpublished  = {Preprint},
}

@inproceedings{patil2025bfcl,
	title={The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models}, 
	author={Patil, Shishir G. and Mao, Huanzhi and Cheng-Jie Ji, Charlie and Yan, Fanjia and Suresh, Vishnu and Stoica, Ion and E. Gonzalez, Joseph},
	booktitle={Forty-second International Conference on Machine Learning},
	year={2025},
}

@inproceedings{jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world GitHub Issues?},
    author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}
          
@article{liu2023lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023},
  eprint={2307.03172},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and David R. So and Jiangwen Han and Zora Tung and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Tobin and Jakub Pachocki and Aastha Kube and V Wojciech Zaremba and Ilya Sutskever},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{geeksforgeeks_cursor_2025,
  title = {How to Use Cursor AI with Examples},
  author = {GeeksforGeeks},
  year = {2025},
  howpublished = {\url{https://www.geeksforgeeks.org/blogs/how-to-use-cursor-ai-with-examples/}}
}

@misc{learncursor_agent_2025,
  title = {Agent AI Assistant | Learn Cursor},
  author = {{Learn Cursor}},
  year = {2025},
  howpublished = {\url{https://learn-cursor.com/en/wiki/user-guide/agent}}
}

@misc{dao2024transformers,
  title={{Transformers are SSMs}: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Tri Dao and Albert Gu},
  year={2024},
  eprint={2405.21060},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  howpublished = {ICML 2024},
}

@misc{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
  author={Albert Gu and Karan Goel and Christopher R{\'e}},
  year={2021},
  eprint={2111.00396},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  howpublished = {ICLR 2022},
}

@misc{borgeaud2022retro,
  title={Improving language models by retrieving from trillions of tokens}, 
  author={Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de las Casas and Aurelia Guy and Jacob Menick and Roman Ring and Tom Hennigan and Saffron Huang and Loren Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and Karen Simonyan and Jack W. Rae and Erich Elsen and Laurent Sifre},
  year={2022},
  eprint={2112.04426},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  howpublished = {ICML 2022},
}

@inproceedings{pan2024llmlingua2,
  title={{LLMLingua-2}: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression},
  author={Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor R{\"u}hle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang and Karl Cobbe and Vineet Kosaraju and Mo Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268531237}
}

@misc{yin2024slimcode,
      title={{SlimCode}: A Model-Agnostic Simplifier for Code}, 
      author={Peng-cheng Yin and Graham Neubig and Wen-tau Yih and Yejin Choi and Alon Halevy},
      year={2024},
      eprint={2405.11196},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chatzianastasis2024soft,
      title={{Soft Tokens, Hard Truths}: Continuous {CoT} for Robust Reasoning}, 
      author={Michail Chatzianastasis and Hamish Ivison and Daniel T. T. O. Po and Alessandro G. Magnani and Jonathan Mallinson and Aliaksei Severyn and Eric Hambro},
      year={2024},
      eprint={2409.19170},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zeng2023agenttuning,
      title={{AgentTuning}: Enabling Generalized Agents with Training on Agent-Specific Trajectories}, 
      author={Aohan Zeng and Mingdao Wang and Xiao-Man Zhang and Silen Naihin and Rui Wen and Pan Lu and Yuxiao Dong and Jie Tang},
      year={2023},
      eprint={2310.12823},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{laskin2024leave,
      title={{Leave No Context Behind}: Efficient Infinite Context Transformers with {Infini-attention}}, 
      author={Misha Laskin and Armen Aghajanyan and Tatsunori Hashimoto and Kelvin Guu and Lajanugen Logeswaran and Sourabh Gupta and Jia Xu and Luke Zettlemoyer and Naman Goyal and Mike Lewis and Ari S. Morcos},
      year={2024},
      eprint={2404.07143},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}