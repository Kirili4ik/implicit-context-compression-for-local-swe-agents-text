@article{zhao2024position,
  title={Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models},
  author={Zhao, Runsong and Liu, Xin and Liu, Xinyu and Huang, Pengcheng and Xiao, Chunyang and Xiao, Tong and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2409.14364},
  year={2024},
  note={\url{https://arxiv.org/abs/2409.14364}}
}

@article{ge2023context,
  title={In-context autoencoder for context compression in a large language model},
  author={Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  journal={arXiv preprint arXiv:2307.06945},
  year={2023},
  note={\url{https://arxiv.org/abs/2307.06945}}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020},
  note={\url{https://arxiv.org/abs/2004.05150}}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020},
  note={\url{https://arxiv.org/abs/2007.14062}}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020},
  note={\url{https://arxiv.org/abs/2006.04768}}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  note={\url{https://arxiv.org/abs/1706.03762}}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018},
  note={\url{https://arxiv.org/abs/1803.02155}}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier},
  note={\url{https://arxiv.org/abs/2104.09864}}
}


@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022},
  note={\url{https://arxiv.org/abs/2106.09685}}
}

@inproceedings{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan},
  booktitle={The eleventh international conference on learning representations},
  year={2022},
  note={\url{https://arxiv.org/abs/2210.03629}}
}


@article{yang2025swe,
  title={Swe-smith: Scaling data for software engineering agents},
  author={Yang, John and Lieret, Kilian and Jimenez, Carlos E and Wettig, Alexander and Khandpur, Kabir and Zhang, Yanzhe and Hui, Binyuan and Press, Ofir and Schmidt, Ludwig and Yang, Diyi},
  journal={arXiv preprint arXiv:2504.21798},
  year={2025},
  note={\url{https://arxiv.org/abs/2504.21798}}
}

@inproceedings{patilberkeley,
  title={The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models},
  author={Patil, Shishir G and Mao, Huanzhi and Yan, Fanjia and Ji, Charlie Cheng-Jie and Suresh, Vishnu and Stoica, Ion and Gonzalez, Joseph E},
  booktitle={Forty-second International Conference on Machine Learning},
  note={\url{https://openreview.net/forum?id=2GmDdhBdDk}}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023},
  note={\url{https://arxiv.org/abs/2310.06770}}
}
          
@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  note={\url{https://arxiv.org/abs/2307.03172}}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021},
  note={\url{https://arxiv.org/abs/2108.07732}}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
  note={\url{https://arxiv.org/abs/2107.03374}}
}


@article{dao2024transformers,
  title={Transformers are ssms: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024},
  note={\url{https://arxiv.org/abs/2405.21060}}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021},
  note={\url{https://arxiv.org/abs/2111.00396}}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR},
  note={\url{https://arxiv.org/abs/2112.04426}}
}

@article{pan2024llmlingua,
  title={Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression},
  author={Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2403.12968},
  year={2024},
  note={\url{https://arxiv.org/abs/2403.12968}}
}

@article{wang2024natural,
  title={Natural is the best: Model-agnostic code simplification for pre-trained large language models},
  author={Wang, Yan and Li, Xiaoning and Nguyen, Tien N and Wang, Shaohua and Ni, Chao and Ding, Ling},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={586--608},
  year={2024},
  publisher={ACM New York, NY, USA},
  note={\url{https://arxiv.org/abs/2405.11196}}
}

@article{butt2025soft,
  title={Soft tokens, hard truths},
  author={Butt, Natasha and Kwiatkowski, Ariel and Labiad, Ismail and Kempe, Julia and Ollivier, Yann},
  journal={arXiv preprint arXiv:2509.19170},
  year={2025},
  note={\url{https://arxiv.org/abs/2509.19170}}
}

@inproceedings{zeng2024agenttuning,
  title={Agenttuning: Enabling generalized agent abilities for llms},
  author={Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={3053--3077},
  year={2024},
  note={\url{https://arxiv.org/abs/2310.12823}}
}

@article{munkhdalai2024leave,
  title={Leave no context behind: Efficient infinite context transformers with infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv preprint arXiv:2404.07143},
  volume={101},
  year={2024},
  note={\url{https://arxiv.org/abs/2404.07143}}
}

@article{liu2024repoqa,
  title={Repoqa: Evaluating long context code understanding},
  author={Liu, Jiawei and Tian, Jia Le and Daita, Vijay and Wei, Yuxiang and Ding, Yifeng and Wang, Yuhan Katherine and Yang, Jun and Zhang, Lingming},
  journal={arXiv preprint arXiv:2406.06025},
  year={2024},
  note={\url{https://arxiv.org/abs/2406.06025}}
}

@misc{chowdhury2024swebenchverified,
  author    = {Chowdhury, Neil and Aung, James and Shern, Chan Jun and Jaffe, Oliver and Sherburn, Dane and Starace, Giulio and Mays, Evan and Dias, Rachel and Aljubeh, Marwan and Glaese, Mia and Jimenez, Carlos E. and Yang, John and Ho, Leyton and Patwardhan, Tejal and Liu, Kevin and Madry, Aleksander},
  title     = {Introducing SWE-bench Verified},
  year      = {2024},
  month     = {August},
  howpublished = {OpenAI Blog},
  url       = {https://openai.com/index/introducing-swe-bench-verified/},
  note = {Accessed: 2025-11-13}
}

@article{song2024agentbank,
  title={Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajectories},
  author={Song, Yifan and Xiong, Weimin and Zhao, Xiutian and Zhu, Dawei and Wu, Wenhao and Wang, Ke and Li, Cheng and Peng, Wei and Li, Sujian},
  journal={arXiv preprint arXiv:2410.07706},
  year={2024},
  note={\url{https://arxiv.org/abs/2410.07706}}
}

@article{yang2024swe,
  title={Swe-agent: Agent-computer interfaces enable automated software engineering},
  author={Yang, John and Jimenez, Carlos E and Wettig, Alexander and Lieret, Kilian and Yao, Shunyu and Narasimhan, Karthik and Press, Ofir},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={50528--50652},
  year={2024},
  note={\url{https://arxiv.org/abs/2405.15793}}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  note={\url{https://aclanthology.org/P02-1040.pdf}}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016},
  note={\url{https://arxiv.org/abs/1606.05250}}
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025},
  note={\url{https://arxiv.org/abs/2505.09388}}
}

@article{weber2024redpajama,
  title={Redpajama: an open dataset for training large language models},
  author={Weber, Maurice and Fu, Dan and Anthony, Quentin and Oren, Yonatan and Adams, Shane and Alexandrov, Anton and Lyu, Xiaozhong and Nguyen, Huu and Yao, Xiaozhe and Adams, Virginia and others},
  journal={Advances in neural information processing systems},
  volume={37},
  pages={116462--116492},
  year={2024},
  note={\url{https://arxiv.org/abs/2411.12372}}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019},
  note={\url{https://arxiv.org/abs/1810.04805}}
}

@misc{jiang2023mistral,
  title = {Announcing Mistral 7B},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Renard Lavaud, Lélio and Lachaux, Marie-Anne and Stock, Pierre and Le Scao, Teven and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Lavril, Victor},
  year = {2023},
  note = {\url{https://mistral.ai/news/announcing-mistral-7b}}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{zhuang2023survey,
  title={A survey on efficient training of transformers},
  author={Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
  journal={arXiv preprint arXiv:2302.01107},
  year={2023}
}

@article{chevalier2023adapting,
  title={Adapting language models to compress contexts},
  author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14788},
  year={2023}
}

@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@misc{githubcopilot,
  title = {{GitHub Copilot}},
  author = {{GitHub}},
  year = {2021},
  howpublished = {\url{https://github.com/features/copilot}}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@misc{pile_dmca,
  title = {The Pile (dataset)},
  author = {Wikipedia},
  year = {2025},
  howpublished = {\url{https://en.wikipedia.org/wiki/The_Pile_(dataset)\#DMCA_takedown}},
  note = {Accessed: 2025-11-26}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}

@misc{anthropic2025claude,
  author    = {Anthropic},
  title     = {Introducing Claude Opus 4.5},
  year      = {2025},
  month     = {November},
  howpublished = {Anthropic Blog},
  url       = {https://www.anthropic.com/news/claude-opus-4-5},
  note = {Accessed: 2025-11-30}
}

@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}