
@misc{zhao_position_2025,
	title = {Position {IDs} {Matter}: {An} {Enhanced} {Position} {Layout} for {Efficient} {Context} {Compression} in {Large} {Language} {Models}},
	shorttitle = {Position {IDs} {Matter}},
	url = {http://arxiv.org/abs/2409.14364},
	doi = {10.48550/arXiv.2409.14364},
	abstract = {Using special tokens (e.g., gist, memory, or compressed tokens) to compress context information is a common practice for large language models (LLMs). However, existing approaches often neglect that position encodings inherently induce local inductive biases in models, causing the compression process to ignore holistic contextual dependencies. We propose {\textbackslash}textbf\{Enhanced Position Layout (EPL)\}, a simple yet effective method that improves the context compression capability of LLMs by only adjusting position IDs, the numerical identifiers that specify token positions. EPL minimizes the distance between context tokens and their corresponding special tokens and at the same time maintains the sequence order in position IDs between context tokens, special tokens, and the subsequent tokens. Integrating EPL into our best performing context compression model results in a 1.9 ROUGE-1 F1 improvement on out-of-domain question answering datasets on average. When extended to multimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for vision compression LLMs.},
	urldate = {2025-10-08},
	publisher = {arXiv},
	author = {Zhao, Runsong and Liu, Xin and Liu, Xinyu and Huang, Pengcheng and Xiao, Chunyang and Xiao, Tong and Zhu, Jingbo},
	month = sep,
	year = {2025},
	note = {arXiv:2409.14364 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/Kirill.Gelvan/Zotero/storage/BKEQSJC5/Zhao et al. - 2025 - Position IDs Matter An Enhanced Position Layout for Efficient Context Compression in Large Language.pdf:application/pdf;Snapshot:/Users/Kirill.Gelvan/Zotero/storage/GLUAX6T4/2409.html:text/html},
}

@misc{ge_-context_2024,
	title = {In-context {Autoencoder} for {Context} {Compression} in a {Large} {Language} {Model}},
	url = {http://arxiv.org/abs/2307.06945},
	doi = {10.48550/arXiv.2307.06945},
	abstract = {We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1\% additional parameters, effectively achieves \$4{\textbackslash}times\$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.},
	urldate = {2025-10-10},
	publisher = {arXiv},
	author = {Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
	month = may,
	year = {2024},
	note = {arXiv:2307.06945 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Kirill.Gelvan/Zotero/storage/UE55FTJ9/Ge et al. - 2024 - In-context Autoencoder for Context Compression in a Large Language Model.pdf:application/pdf;Snapshot:/Users/Kirill.Gelvan/Zotero/storage/PSYMBTWP/2307.html:text/html},
}

@misc{beltagy_longformer_2020,
  title         = {Longformer: The Long-Document Transformer},
  author        = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year          = {2020},
  eprint        = {2004.05150},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{zaheer_bigbird_2020,
  title         = {Big Bird: Transformers for Longer Sequences},
  author        = {Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  year          = {2020},
  eprint        = {2007.14062},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{child_sparse_2019,
  title         = {Generating Long Sequences with Sparse Transformers},
  author        = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
  year          = {2019},
  eprint        = {1904.10509},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{katharopoulos_transformers_2020,
  title         = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author        = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Fran{\c c}ois Fleuret},
  year          = {2020},
  eprint        = {2006.16236},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{choromanski_performer_2021,
  title         = {Rethinking Attention with Performers},
  author        = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tam{\'a}s Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy J. Colwell and Adrian Weller},
  year          = {2021},
  eprint        = {2009.14794},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{wang_linformer_2020,
  title         = {Linformer: Self-Attention with Linear Complexity},
  author        = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
  year          = {2020},
  eprint        = {2006.04768},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{lewis_rag_2020,
  title         = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP}},
  author        = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{"u}ttler and Mike Lewis and Wen-tau Yih and Tim Rockt{"a}schel and Sebastian Riedel and Douwe Kiela},
  year          = {2020},
  eprint        = {2005.11401},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{li_prefix_2021,
  title         = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author        = {Xiang Lisa Li and Percy Liang},
  year          = {2021},
  eprint        = {2101.00190},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{lester_prompt_2021,
  title         = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  author        = {Brian Lester and Rami Al-Rfou and Noah Constant},
  year          = {2021},
  eprint        = {2104.08691},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{zhang_pegasus_2020,
  title         = {PEGASUS: Pre-training with Extracted Gaps Sentences for Abstractive Summarization},
  author        = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
  year          = {2020},
  eprint        = {1912.08777},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{attentionrag_2025,
  title         = {AttentionRAG: Attention-Guided Context Pruning for {RAG}},
  author        = {Anonymous},
  year          = {2025},
  eprint        = {2503.10720},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
}

@misc{block_attention_rag_2024,
  title         = {Block-Attention for Efficient {RAG}},
  author        = {Anonymous},
  year          = {2024},
  eprint        = {2409.15355},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
}

@inproceedings{vaswani_attention_2017,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017}
}

@inproceedings{shaw_relative_2018,
  title     = {Self-Attention with Relative Position Representations},
  author    = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  booktitle = {NAACL-HLT},
  year      = {2018}
}

@misc{su_roformer_2021,
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author        = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  year          = {2021},
  eprint        = {2104.09864},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{hu2021lora,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  year          = {2021},
  eprint        = {2106.09685},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{yao_react_2022,
  title         = {ReAct: Synergizing Reasoning and Acting in Language Models},
  author        = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year          = {2022},
  eprint        = {2210.03629},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{schick_toolformer_2023,
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  author        = {Timo Schick and Jane Dwivedi-Yu and Noah Constant and Te-Yen Wu and Xi Victoria Lin and Sushant Prakash and Urvashi Khandelwal and Ben Athiwaratkun and Kuang-Huei Lee and Ed Chi and Mike Lewis and Colin Raffel and Holger Schwenk and Sebastian Ruder},
  year          = {2023},
  eprint        = {2302.04761},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{nakano_webgpt_2021,
  title         = {WebGPT: Browser-assisted question-answering with human feedback},
  author        = {Reiichiro Nakano and Jacob Pfau and Laria Reynolds and Kyle McDonell and Joar Skalse and Olli Hansson and Owain Evans and William Saunders and Matthew Rahtz and Amanda Askell and John Schulman and Jan Leike},
  year          = {2021},
  eprint        = {2112.09332},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{swesmith,
  title         = {SWE-smith: Agent Setup and Prompt for SWE-bench Verified},
  note          = {Describes the SWE-smith tool protocol and prompt for SWE-bench Verified. Prompt text included in Appendix~\ref{app:swe-smith-prompt}.},
  year          = {2024},
  howpublished  = {Preprint},
}

@inproceedings{patil2025bfcl,
	title={The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models}, 
	author={Patil, Shishir G. and Mao, Huanzhi and Cheng-Jie Ji, Charlie and Yan, Fanjia and Suresh, Vishnu and Stoica, Ion and E. Gonzalez, Joseph},
	booktitle={Forty-second International Conference on Machine Learning},
	year={2025},
}

@inproceedings{jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world GitHub Issues?},
    author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}
          