
@misc{zhao_position_2025,
	title = {Position {IDs} {Matter}: {An} {Enhanced} {Position} {Layout} for {Efficient} {Context} {Compression} in {Large} {Language} {Models}},
	shorttitle = {Position {IDs} {Matter}},
	url = {http://arxiv.org/abs/2409.14364},
	doi = {10.48550/arXiv.2409.14364},
	abstract = {Using special tokens (e.g., gist, memory, or compressed tokens) to compress context information is a common practice for large language models (LLMs). However, existing approaches often neglect that position encodings inherently induce local inductive biases in models, causing the compression process to ignore holistic contextual dependencies. We propose {\textbackslash}textbf\{Enhanced Position Layout (EPL)\}, a simple yet effective method that improves the context compression capability of LLMs by only adjusting position IDs, the numerical identifiers that specify token positions. EPL minimizes the distance between context tokens and their corresponding special tokens and at the same time maintains the sequence order in position IDs between context tokens, special tokens, and the subsequent tokens. Integrating EPL into our best performing context compression model results in a 1.9 ROUGE-1 F1 improvement on out-of-domain question answering datasets on average. When extended to multimodal scenarios, EPL leads to an average accuracy gain of 2.6 points for vision compression LLMs.},
	urldate = {2025-10-08},
	publisher = {arXiv},
	author = {Zhao, Runsong and Liu, Xin and Liu, Xinyu and Huang, Pengcheng and Xiao, Chunyang and Xiao, Tong and Zhu, Jingbo},
	month = sep,
	year = {2025},
	note = {arXiv:2409.14364 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/Kirill.Gelvan/Zotero/storage/BKEQSJC5/Zhao et al. - 2025 - Position IDs Matter An Enhanced Position Layout for Efficient Context Compression in Large Language.pdf:application/pdf;Snapshot:/Users/Kirill.Gelvan/Zotero/storage/GLUAX6T4/2409.html:text/html},
}

@misc{ge_-context_2024,
	title = {In-context {Autoencoder} for {Context} {Compression} in a {Large} {Language} {Model}},
	url = {http://arxiv.org/abs/2307.06945},
	doi = {10.48550/arXiv.2307.06945},
	abstract = {We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1\% additional parameters, effectively achieves \$4{\textbackslash}times\$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.},
	urldate = {2025-10-10},
	publisher = {arXiv},
	author = {Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
	month = may,
	year = {2024},
	note = {arXiv:2307.06945 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Kirill.Gelvan/Zotero/storage/UE55FTJ9/Ge et al. - 2024 - In-context Autoencoder for Context Compression in a Large Language Model.pdf:application/pdf;Snapshot:/Users/Kirill.Gelvan/Zotero/storage/PSYMBTWP/2307.html:text/html},
}
