% ========================================
% APPENDIX SECTION
% ========================================
\appendix
\chapter{Appendix}


% ========================================
% APPENDIX SECTION A.1: TRAINING DETAILS AND HYPERPARAMETERS
% ========================================
\section{Training Details and Hyperparameters}
\label{app:training_details}

\subsection{Pretraining Configuration}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base Model & Qwen3-8B \\
        Dataset & SlimPajama-6B \\
        Learning Rate & $1 \times 10^{-4}$ \\
        Batch Size & 1 \\
        Gradient Accumulation & 8 \\
        Training Steps & $\approx$100,000 \\
        Memory Size & 256 tokens (4× compression) \\
        LoRA Rank & 128 \\
        LoRA Target Modules & q\_proj, v\_proj \\
        Optimizer & AdamW \\
        Warmup Steps & 300 \\
        Hardware & 1× NVIDIA H200 GPU \\
        Training Time & $\approx$1 day 15 hours \\
        \bottomrule
    \end{tabular}
    \caption{Pretraining hyperparameters and configuration}
    \label{tab:pretrain_config}
\end{table}

\subsection{Fine-tuning Configuration}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base Model & Qwen3-8B \\
        Dataset & SWE-bench trajectories \\
        Learning Rate & $5 \times 10^{-5}$ \\
        Batch Size & 1 \\
        Gradient Accumulation & 1 \\
        Training Steps & $\approx$150,000 \\
        Memory Size & 256 tokens (4× compression) \\
        LoRA Rank & 128 \\
        LoRA Target Modules & q\_proj, v\_proj \\
        Optimizer & AdamW \\
        Warmup Steps & 250 \\
        Hardware & 1× NVIDIA H200 GPU \\
        Training Time & $\approx$3 days \\
        \bottomrule
    \end{tabular}
    \caption{Fine-tuning hyperparameters and configuration}
    \label{tab:finetune_config}
\end{table}

\subsection{Reproducibility Resources}

To ensure full reproducibility, we publish our complete implementation including:
\begin{itemize}
    \item Complete ICAE framework for both pretraining and fine-tuning phases (\url{https://github.com/JetBrains-Research/icae})
    \item Full Weights \& Biases experiment logs for pretraining: \url{https://wandb.ai/kirili4ik/icae-pretraining}
    \item Full Weights \& Biases experiment logs for fine-tuning: \url{https://wandb.ai/kirili4ik/icae-swebench-finetune}
    \item Pretrained model checkpoints achieving 95\% reconstruction BLEU: TODO
    \item Fine-tuned models that outperform uncompressed baselines on SQuAD: TODO
\end{itemize}


% ========================================
% APPENDIX SECTION A.2: PROFILING SETUP AND LATENCY MEASUREMENT
% ========================================
\section{Profiling Setup and Latency Measurement}

Technical details of the test machine and runtime configuration used for latency measurements.


% ========================================
% APPENDIX SECTION A.4: DETAILED EVALUATION TABLES
% ========================================
\section{Detailed Evaluation Tables}

Comprehensive tables of model performance, including token-wise accuracy, mean tool-call time, and resolved issues for various ICAE \cite{ge_-context_2024} variants (e.g., Qwen-LoRA-FT, ICAE (Qwen-LoRA-FT) Qwen).
