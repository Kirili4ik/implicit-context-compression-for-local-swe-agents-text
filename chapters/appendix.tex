% ========================================
% APPENDIX SECTION
% ========================================
\appendix
\chapter{Appendix}


\section{On the Use of AI}

I \textbf{have} used generative AI to help me write the text for this work.\\
I \textbf{have} used generative AI to help me write the code for the experiments.\\
I \textbf{have not} used AI in order to create new experiments, nor for any goals, research questions, hypotheses, etc.


\section{SWE-smith Prompt}
\label{app:swe-smith-prompt}

The following prompt is from the SWE-smith paper \cite{yang2025swe}.

\begin{tcolorbox}[title={SWE-smith Prompt},
    colback=white, colframe=black, fonttitle=\bfseries,
    breakable, sharp corners=south, enhanced jigsaw]
  \begin{Verbatim}[breaklines=true, obeytabs=false,breaksymbol={},breakindent=0pt,fontsize=\tiny]
    You are a helpful assistant that can interact with a computer to solve tasks.
    <IMPORTANT>
    * If user provides a path, you should NOT assume it's relative to the current working directory. Instead, you should explore the file system to find the file before working on it.
    </IMPORTANT>
    
    You have access to the following functions:
    
    ---- BEGIN FUNCTION #1: bash ----
    Description: Execute a bash command in the terminal.
    
    Parameters:
      (1) command (string, required): The bash command to execute. Can be empty to view additional logs when previous exit code is `-1`. Can be `ctrl+c` to interrupt the currently running process.
    ---- END FUNCTION #1 ----
    
    ---- BEGIN FUNCTION #2: submit ----
    Description: Finish the interaction when the task is complete OR if the assistant cannot proceed further with the task.
    No parameters are required for this function.
    ---- END FUNCTION #2 ----
    
    ---- BEGIN FUNCTION #3: str_replace_editor ----
    Description: Custom editing tool for viewing, creating and editing files
    * State is persistent across command calls and discussions with the user
    * If `path` is a file, `view` displays the result of applying `cat -n`. If `path` is a directory, `view` lists non-hidden files and directories up to 2 levels deep
    * The `create` command cannot be used if the specified `path` already exists as a file
    * If a `command` generates a long output, it will be truncated and marked with `<response clipped>`
    * The `undo_edit` command will revert the last edit made to the file at `path`
    
    Notes for using the `str_replace` command:
    * The `old_str` parameter should match EXACTLY one or more consecutive lines from the original file. Be mindful of whitespaces!
    * If the `old_str` parameter is not unique in the file, the replacement will not be performed. Make sure to include enough context in `old_str` to make it unique
    * The `new_str` parameter should contain the edited lines that should replace the `old_str`
    
    Parameters:
      (1) command (string, required): The commands to run. Allowed options are: `view`, `create`, `str_replace`, `insert`, `undo_edit`.
    Allowed values: [`view`, `create`, `str_replace`, `insert`, `undo_edit`]
      (2) path (string, required): Absolute path to file or directory, e.g. `/repo/file.py` or `/repo`.
      (3) file_text (string, optional): Required parameter of `create` command, with the content of the file to be created.
      (4) old_str (string, optional): Required parameter of `str_replace` command containing the string in `path` to replace.
      (5) new_str (string, optional): Optional parameter of `str_replace` command containing the new string (if not given, no string will be added). Required parameter of `insert` command containing the string to insert.
      (6) insert_line (integer, optional): Required parameter of `insert` command. The `new_str` will be inserted AFTER the line `insert_line` of `path`.
      (7) view_range (array, optional): Optional parameter of `view` command when `path` points to a file. If none is given, the full file is shown. If provided, the file will be shown in the indicated line number range, e.g. [11, 12] will show lines 11 and 12. Indexing at 1 to start. Setting [start_line, -1] shows all lines from start_line to the end of the file.
    ---- END FUNCTION #3 ----
    
    
    If you choose to call a function ONLY reply in the following format with NO suffix:
    
    Provide any reasoning for the function call here.
    <function=example_function_name>
    <parameter=example_parameter_1>value_1</parameter>
    <parameter=example_parameter_2>
    This is the value for the second parameter
    that can span
    multiple lines
    </parameter>
    </function>
    
    <IMPORTANT>
    Reminder:
    - Function calls MUST follow the specified format, start with <function= and end with </function>
    - Required parameters MUST be specified
    - Only call one function at a time
    - Always provide reasoning for your function call in natural language BEFORE the function call (not after)
    </IMPORTANT>
  \end{Verbatim}
  \end{tcolorbox}

\section{SWE-smith First User Message}
\label{app:swe-smith-first-user-message}

\begin{tcolorbox}[title={SWE-smith First User Message},
    colback=white, colframe=black, fonttitle=\bfseries,
    breakable, sharp corners=south, enhanced jigsaw]
    \begin{Verbatim}[breaklines=true, obeytabs=false,breaksymbol={},breakindent=0pt,fontsize=\tiny]
<|im_start|>user
<uploaded_files>
<repo_path>*
</uploaded_files>
I've uploaded a python code repository in the directory /mnt/shared-fs/gelvan/swe-agent-distillation. Consider the following PR description:

<pr_description>
<issue_description>*
</pr_description>

Can you help me implement the necessary changes to the repository so that the requirements specified in the <pr_description> are met?
I've already taken care of all changes to any of the test files described in the <pr_description>. This means you DON'T have to modify the testing logic or any of the tests in any way!
Your task is to make the minimal changes to non-tests files in the /mnt/shared-fs/gelvan/swe-agent-distillation directory to ensure the <pr_description> is satisfied.
Follow these steps to resolve the issue:
1. As a first step, it might be a good idea to find and read code relevant to the <pr_description>
2. Create a script to reproduce the error and execute it with `python <filename.py>` using the bash tool, to confirm the error
3. Edit the source code of the repo to resolve the issue
4. Rerun your reproduce script and confirm that the error is fixed!
5. Think about edgecases and make sure your fix handles them as well
Your thinking should be thorough and so it's fine if it's very long.<|im_end|>
  \end{Verbatim}
\end{tcolorbox}
*\texttt{<issue\_description>} is replaced with the specific problem description for each SWE-bench task.
*\texttt{<repo\_path>} is replaced with the path to the repository.

  

  
\section{ICAE In-Context Ability Proof-of-Concept}
\label{app:icae-proof}

\begin{tcolorbox}[title={ICAE Proof-of-Concept Example},
    colback=white, colframe=black, fonttitle=\bfseries,
    breakable, sharp corners=south, enhanced jigsaw]
  \begin{Verbatim}[breaklines=true, obeytabs=false,breaksymbol={},breakindent=0pt,fontsize=\tiny,commandchars=\\\{\}]
<|im_start|>system
<system_prompt>*
<|im_end|>

<|im_start|>user
<uploaded_files>
/mnt/shared-fs/gelvan/swe-agent-distillation
</uploaded_files>
I've uploaded a python code repository in the directory /mnt/shared-fs/gelvan/swe-agent-distillation. Consider the following PR description:
<pr_description>
\colorbox{yellow}{Your task is to read file README.md and run the commands that are presented in that file for the setup. when i run them i get errors.}
</pr_description>
<fixed_part_of_the_first_user_message>*<|im_end|>

<|im_start|>assistant
$ str_replace_editor --command view --path /mnt/shared-fs/gelvan/swe-agent-distillation/README.md<|im_end|>

<|im_start|>user
\colorbox{yellow}{<MEMORY_TOKEN> x 512}<|im_end|>

<|im_start|>assistant
$ cat /mnt/shared-fs/gelvan/swe-agent-distillation/requirements.txt<|im_end|>

<|im_start|>user
There is no file named requirements.txt<|im_end|>

<|im_start|>assistant
$ str_replace_editor --command view --path /mnt/shared-fs/gelvan/swe-agent-distillation<|im_end|>

<|im_start|>user
You have no access to this directory.<|im_end|>

<|im_start|>assistant
$ ls -la /mnt/shared-fs/gelvan/swe-agent-distillation<|im_end|>

<|im_start|>user
You have no access to this directory. You have to call a command from README.md to install.<|im_end|>

<|im_start|>assistant
$ str_replace_editor --command view --path /mnt/shared-fs/gelvan/swe-agent-distillation/README.md<|im_end|>

<|im_start|>user
\colorbox{yellow}{<MEMORY_TOKEN> x 512}<|im_end|>

<|im_start|>assistant
\colorbox{yellow}{$ cat /mnt/shared-fs/gelvan/swe-agent-distillation/.my_secret_requirements.txt}<|im_end|>
  \end{Verbatim}
\end{tcolorbox}
This example demonstrates a hand-crafted scenario, closely related to SWE-bench, designed to test the model's ability to comprehend and utilize decompressed data from its memory. The highlighted part shows the model successfully identifying a hidden requirements file by using the context provided in its compressed memory.
*\texttt{<system\_prompt>} is from Appendix~\ref{app:swe-smith-prompt}.

\section{Ablation Experiment: Disabling Thinking}
\label{sec:disabling_thinking}

The Qwen3 model family provides a mechanism to toggle its "thinking" mode, which is designed for complex reasoning.
For our agentic experiments, where a tool-call generation is prioritized, we decided to disable this feature not to overcomplicate the pipeline.
The official recommendation for disabling thinking involves prefixing the generation prompt with a specific token sequence, \texttt{<think>\textbackslash n\textbackslash n</think>}, which signals to the model that the "thinking" step has already occurred and is empty.
Crucially, for multi-turn interactions, this prefix should be ephemeral: it is added for generation and then omitted from the conversation history to prevent it from influencing subsequent turns.

In an early stage of our experiments, we explored the model's sensitivity to this prompting convention by deviating from the recommended protocol.
Instead of removing the thinking prefix from the history, we allowed it to accumulate with each agent step.
This resulted in a setup where the context for generating action \(a_k\) contained not only the history of actions and observations but also \(k-1\) instances of the thinking-disabling prefix.
While unintentional, this created a distinct experimental condition that we analyzed for its impact on model performance.

Table~\ref{tab:thinking_variants} compares the performance of key model configurations under both the official "Clean" prompting protocol and our "Cumulative" prompting experiment.

\begin{table}[h]
    \centering
    \small
    \caption{Comparison of prompting strategies for disabling thinking in Qwen3-8B. "Cumulative" refers to accumulating the `\texttt{<think>...}` prefix in the history, while "Clean" follows the official recommendation of removing it after each step.}
    \label{tab:thinking_variants}
    \begin{tabular}{llc}
        \toprule
        \textbf{Configuration} & \textbf{Prompting Strategy} & \textbf{Accuracy $\uparrow$} \\
        \midrule
        \texttt{Baseline} & Cumulative & 0.9000 \\
        \texttt{Baseline} & Clean & 0.8967 \\
        \addlinespace
        \texttt{ICAE-PT+FT} & Cumulative & 0.9089 \\
        \texttt{ICAE-PT+FT} & Clean & 0.9020 \\
        \bottomrule
    \end{tabular}
\end{table}

The results present an unexpected finding.
The "Cumulative" prompting strategy, despite polluting the context with repetitive, non-semantic tokens, yielded slightly higher token-wise accuracy compared to the "Clean" approach for both the \texttt{baseline} Qwen model (0.9000 vs. 0.8967) and the \texttt{ICAE-PT+FT} compressed model (0.9089 vs. 0.9020).
Note that we, of course, do not calculate the accuracy on the thinking tokens.
We hypothesize that the repeated, structured nature of the accumulated prefixes might enable the model's attention mechanism to process the context more efficiently, or that the model learns to largely ignore these predictable tokens, leading to a marginal improvement in next-token prediction.

However, it is important to note that these differences are small and might be attributable to randomness in the evaluation process.
Due to resource constraints, we did not conduct multiple runs to calculate standard deviations for these measurements, which would be necessary to establish statistical significance.

Consequently, all other experiments reported in this work, including the main results in Table~\ref{tab:qwen_icae_variants_absolute}, were conducted using the official "Clean" prompting methodology to ensure the validity and reproducibility of our findings.
This ablation study may be of interest to future work involving the Qwen3 model family.

\section{Training Details and Hyperparameters}
\label{app:training_details}

\subsection{Pretraining Configuration}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base Model & Qwen3-8B \\
        Dataset & SlimPajama-6B \\
        Learning Rate & $1 \times 10^{-4}$ \\
        Batch Size & 1 \\
        Gradient Accumulation & 8 \\
        Training Steps & $\approx$100,000 \\
        Memory Size & 256 tokens (4× compression) \\
        LoRA $r$ & 128 \\
        LoRA $\alpha$ & 32 \\
        LoRA Target Modules & q\_proj, v\_proj \\
        Optimizer & AdamW \\
        Warmup Steps & 300 \\
        Hardware & 1× NVIDIA H200 GPU \\
        Training Time & $\approx$1 day 15 hours \\
        \bottomrule
    \end{tabular}
    \caption{Pretraining hyperparameters and configuration}
    \label{tab:pretrain_config}
\end{table}

\subsection{Fine-tuning Configuration}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base Model & Qwen3-8B \\
        Dataset & SWE-bench trajectories \\
        Learning Rate & $5 \times 10^{-5}$ \\
        Batch Size & 1 \\
        Gradient Accumulation & 1 \\
        Training Steps & $\approx$150,000 \\
        Memory Size & 256 tokens (4× compression) \\
        LoRA $r$ & 128 \\
        LoRA $\alpha$ & 32 \\
        LoRA Target Modules & q\_proj, v\_proj \\
        Optimizer & AdamW \\
        Warmup Steps & 250 \\
        Hardware & 1× NVIDIA H200 GPU \\
        Training Time & $\approx$3 days \\
        \bottomrule
    \end{tabular}
    \caption{Fine-tuning hyperparameters and configuration}
    \label{tab:finetune_config}
\end{table}

\subsection{SWE-bench Inference Configuration}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Dataset & SWE-bench Verified\\
        Temperature & 1.0 \\
        Per-instance Call Limit & 75 \\
        Max Input Tokens & 32768 \\
        \bottomrule
    \end{tabular}
    \caption{SWE-bench inference configuration}
    \label{tab:inference_config}
\end{table}

\subsection{Profiling Setup}

All experiments were conducted on a single server equipped with an Intel processor and 1× NVIDIA H200 GPU. We utilized only one GPU for all training and evaluation tasks to ensure consistency across experiments.

\subsection{Reproducibility Resources}
To ensure full reproducibility, we provide our complete implementation\footnote{\url{https://github.com/JetBrains-Research/icae}}, full Weights \& Biases experiment logs for pretraining\footnote{\url{https://wandb.ai/kirili4ik/icae-pretraining}} and fine-tuning\footnote{\url{https://wandb.ai/kirili4ik/icae-swebench-finetune}}, and model checkpoints\footnote{\textbf{TODO}}\footnote{\textbf{TODO}}.

\textbf{TODO: 4 is Pretrained model checkpoints achieving 95\% reconstruction BLEU:}

\textbf{TODO: 5 is Fine-tuned models that outperform uncompressed baselines on SQuAD:}