% ========================================
% CHAPTER 6: EXPERIMENTS AND EVALUATION (ACTUAL SETUPS AND RESULTS)
% ========================================
\chapter{Experiments}
\label{cha:experiments}

This chapter documents the experimental results.
First, training-free and projection-based compression methods are evaluated.
Next, the pretraining of the In-Context Autoencoder (ICAE) is described alongside results on general text reconstruction.
Subsequently, fine-tuning performance is assessed on SQuAD and later on RepoQA.
The chapter concludes with an evaluation on the SWE-bench Verified dataset within an agentic software engineering setting.

%
\section{Feasibility of Training-Free Context Condensation}

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/ser111.pdf}
        \caption{Original approach}
        \label{fig:ser1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/ser2.pdf}
        \caption{Our modification variant (soft embedding, online)}
        \label{fig:ser2}
    \end{subfigure}
    \caption{A comparison of token processing pathways. The left image illustrates the standard approach where discrete tokens are converted to embeddings. The right image shows our modified method, which injects continuous soft embeddings directly into the model, bypassing the usual tokenization and embedding lookup stages.}
    \label{fig:ser_combined}
\end{figure}


We first investigate whether meaningful context compression can be achieved without any model training.
These initial experiments explore methods for replacing discrete tokens with continuous representations, testing the hypothesis that simple embedding aggregation can preserve essential information for downstream tasks.

We explore two primary settings for training-free condensation.
In the \textbf{hard embedding} setting, discrete tokens are represented as one-hot vectors that index the input embedding matrix.
For condensation, we compute the elementwise mean of these vectors.
In the \textbf{soft-embedding} setting, we bypass the argmax operation and token lookup, feeding a continuous mixture of embeddings directly to the model.
Figure~\ref{fig:ser1} illustrates the standard token processing pathway, while Figure~\ref{fig:ser2} depicts our modification, which injects these continuous embeddings directly.

\paragraph{Online soft-embedding}
The online pathway is implemented by bypassing token sampling and the embedding lookup.
After running a standard decoding step to obtain logits, we compute the corresponding expected embedding and insert this continuous vector directly as the input for the next step using KV-cache manipulation.
This approach is illustrated in Figure~\ref{fig:ser2}.
The goal is to assess whether context can be compressed without trained adapters.

\paragraph{Regenerate-LLM offline}
To mitigate potential collapse from iterative generation, we also explored an offline method.
In this approach, we prompt the model to reproduce a given input sequence under teacher forcing and record all output embeddings at each step.
At inference time, these saved embeddings are reused as the context representation, avoiding online recomputation.

\paragraph{Results}
Table~\ref{tab:avg_variants} reports SQuAD performance under these condensation strategies.
The results establish that replacing discrete tokens with untrained condensed mixtures, whether generated online or offline, substantially degrades performance on the QA task.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Setting (SQuAD), context embed} &
        \textbf{Exact Match} & \textbf{F1} \\
        \midrule
        Baseline — hard tokens         & \textbf{0.58} & \textbf{0.71} \\
        Hard embedded, avg ×2          & 0.09 & 0.21 \\
        Soft embedded online, avg ×2          & 0.05 & 0.11 \\
        Soft embedded \text{Regenerate-LLM}, avg ×2          & 0.07 & 0.16 \\
        \bottomrule
    \end{tabular}
    \caption{Performance of training-free context condensation on SQuAD. This table compares the baseline (hard tokens) against three condensation methods: hard embedding averaging, soft embedding with online generation, and soft embedding with offline regeneration averaging.}
    \label{tab:avg_variants}
\end{table}

\section{Learning Projections for Context Condensation}

We also investigate whether a trained, low-capacity projection module could learn to compress adjacent embedding vectors while preserving task-relevant information.
We explore several architectural variants to map a pair of concatenated hidden vectors $[e_{2t-1}; e_{2t}]$ from $\mathbb{R}^{2d}$ to a single vector in $\mathbb{R}^{d}$.
These variants included a simple linear projector, a shallow non-linear MLP (one to two layers with GELU activation), and a full BERT encoder \cite{devlin2019bert} to provide a higher-capacity compression mechanism.



\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{graphs/squad_val_loss.pdf}
        \caption{Loss on validation set}
        \label{fig:squad_val_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{graphs/squad_f1_score.pdf}
        \caption{F1 score on validation set}
        \label{fig:squad_f1_score}
    \end{subfigure}
    \caption{SQuAD validation metrics for projection-based compression. The left plot shows the validation loss, which flattens immediately despite the training loss decreasing. The right plot displays F1 scores that remain far below the baseline. We explored numerous variants, including different learning rates, BERT encoder, varying compression ratios, and extended training durations. While all configurations successfully overfit the training data, none generalized to the validation set, demonstrating that simple projection methods fail to preserve task-relevant information.}
    \label{fig:squad_metrics}
\end{figure}

All experiments used the SQuAD context-embedding setting, with the base Qwen3-8B model frozen.
Only the parameters of the projection module were trained via token-level cross-entropy on the answer generation task.
Across all architectural variants, the models failed to generalize.
As shown in Figure~\ref{fig:squad_metrics}, while the models were able to overfit to the training data, the validation loss flattened almost immediately.
The resulting F1 scores remained significantly below the uncompressed baseline, demonstrating that these simple projection methods could not recover the performance lost to compression.

We also explored several modifications to the training protocol through systematic ablation studies:
We varied the projection type (linear versus 1- or 2-layer MLP), normalization schemes (pre/post LayerNorm, scale-preserving residual gates), regularization techniques (weight decay, dropout), and the decision to re-project via vocabulary space versus staying in hidden space.
Additionally, we experimented with unfreezing the token embedding table while keeping the main transformer blocks frozen.
None of these changes meaningfully altered the outcome; the models were consistently unable to train and generalize to the SQuAD task.

The failure of these methods suggests two potential underlying issues.
First, the embedding manifold may possess a non-smooth or complex geometric structure that is disrupted by simple linear or shallow non-linear projections, leading to irreversible information loss that the frozen decoder cannot overcome.
Second, even the higher-capacity BERT encoder may lack the expressive power to learn a sufficiently meaning-preserving compression when paired with a much larger, frozen decoder.

\section{ICAE Pretraining and Results on General Text Reconstruction}
\label{sec:icae_pretraining_results}

This section details the pretraining phase of our In-Context Autoencoder, hereafter referred to as \texttt{ICAE-PT}.
The methodology for this pretraining stage is conceptually outlined in Chapter~\ref{cha:methodology}.
The primary objective of this phase is to train the encoder to generate compressed representations from which the original text can be accurately reconstructed.


The pretraining stage was performed on the dataset SlimPajama-6B \cite{weber2024redpajama} (detailed in Section~\ref{sec:datasets}) using a combination of autoencoding and language modeling objectives.
Following the original ICAE methodology, the training objective was a 50/50 mix of an autoencoding (AE) task, signaled by a special \texttt{<AE>} token, and a language modeling (LM) task, which used no special token.
The AE process is conceptually illustrated in Figure~\ref{fig:icae}.

In Figure~\ref{fig:loss_final} you can notice the sudden drop of the loss.
This is a phenomenon found in \cite{zhao2024position}.
It appears due to the modification of positional encodings for the memory tokens.
We see the effect being the same as described by the authors.
In our experiments, it only appears if we apply the Position ID manipulation (see )


\begin{figure}[hbt]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/pretraining_loss_final.pdf}
      \caption{}
      \label{fig:loss_final}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/pretraining_bleu_final.pdf}
      \caption{}
      \label{fig:bleu_final}
  \end{subfigure}
  \caption{Pretraining evaluation metrics across different model configurations. The left plot displays the validation loss, showing a characteristic sharp drop attributed to the positional encoding adjustments for memory tokens. The right plot presents the BLEU scores for text reconstruction, which improve as training progresses. These metrics together illustrate the effectiveness of the pretraining phase.
  }
  \label{fig:pretraining-metrics-combined}
\end{figure}

\begin{table}[h]
    \centering
    \small
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{lll c}
        \toprule
        \textbf{Run} & \textbf{Checkpoint (\# steps)} & \textbf{Compression} & \textbf{BLEU (mean, n=100)} \\
        \midrule
        Qwen3-8B (no ICAE) & 18k & $\times 1$ & 0.867 \\
        \hline
        \multicolumn{4}{|l|}{\hspace{1em}\textit{10M tokens subset}} \\
        \hline
        \texttt{ICAE-PT} & 9k & $\times 4$ & 0.909 \\
        \texttt{ICAE-PT} & 12k & $\times 4$ & \underline{0.942} \\
        \texttt{ICAE-PT} & 18k & $\times 4$ & 0.902 \\
        \hline
        \multicolumn{4}{|l|}{\hspace{1em}\textit{1B tokens subset}} \\
        \hline
        \texttt{ICAE-PT} & 9k  & $\times 4$ & 0.936 \\
        \texttt{ICAE-PT} & 12k (main) & $\times 4$ & \textbf{0.964} \\
        \texttt{ICAE-PT} & 18k & $\times 4$ & 0.928 \\
        \bottomrule
    \end{tabular}
    \caption{Autoencoding reconstruction BLEU scores on SQuAD contexts. This table compares the reconstruction quality for models pretrained (PT) on different data subsets (10M vs. 1B tokens of SlimPajama-6B) at various checkpoints.
    The checkpoint numbers (9k, 12k, 18k) refer to the number of training steps during pretraining. The run marked (main) is used for all the fine-tunings later. Note that here, pretraining is done via LoRA as well.
    }
    \label{tab:ae_bleu_squad_ours}
\end{table}

We evaluate \texttt{ICAE-PT} autoencoding (AE) pretraining using Qwen3-8B as the base model.
During AE, the encoder compresses input contexts at a fixed $\times 4$ ratio (specifically, $1024\!\to\!256$ tokens on average), and the decoder reconstructs the original text.
We report BLEU on SQuAD contexts tested on 100 samples.
The checkpoint using 1B tokens for 12k steps is selected as the main run and used for fine-tuning.

We have also experimented with compressing 16 tokens into 4 on average instead of 1024 into 256.
This achieved a higher BLEU (\(\approx 0.982\)). 
Notably, none of the 100-sample AE scores reach \(\approx 0.99\).
This may be acceptable for general text, but could be problematic for code, where near-lossless reconstruction is likely a prerequisite for downstream stability.


\medskip
Below, we show a short example where we attempted to reconstruct the README.md file of the SWE-agent project.
The difference is highlighted in yellow.
\begin{tcolorbox}[title={Autoencoding Reconstruction Failure Example},
    colback=white, colframe=black, fonttitle=\bfseries,
    breakable, sharp corners=south, enhanced jigsaw]
  \begin{Verbatim}[breaklines=true, obeytabs=false,breaksymbol={},breakindent=0pt,fontsize=\small,commandchars=\\\{\}]
Original:
<p align="center">
<a href="https://swe-agent.com/latest/">
<strong>Documentation</strong></a>&nbsp;
...

Reconstructed:
<p align="center">
<a href="https://swe-agent.com\colorbox{yellow}{/agent/}latest/">
<strong>Documentation</strong></a>&nbsp;
...
  \end{Verbatim}
\end{tcolorbox}
\label{ex:ae-readme-recon}

\noindent Even in the very start of the text, the difference is noticeable: the hallucinated \texttt{/agent/} path segment in the URL, which could break navigation in a coding task.

In line with internal feedback, these AE findings suggest that the current pretrain/fine-tuning mixes undertrain the model on code: AE BLEU for code should approach text-level (near 1.0) to avoid even small inaccuracies (e.g., link/variable name substitutions).

\section{ICAE Fine-Tuning and Results on Question Answering (SQuAD)}

We conduct fine-tuning experiments on the Stanford Question Answering Dataset (SQuAD) \cite{rajpurkar2016squad}.
The rationale for choosing SQuAD over the PWC dataset used in the original ICAE work is detailed in Chapter~\ref{cha:evaluation}.

During fine-tuning on SQuAD, the encoder compresses the context while the question remains uncompressed ("compressed-text -> uncompressed-question" format).
The decoder generates answers from this mixed representation.
We use identical LoRA hyperparameters as in \cite{ge2023context}.

To establish a comprehensive evaluation baseline, we compare four distinct model configurations that systematically vary the training procedure and compression strategy.
First, we evaluate the base Mistral-7B model \cite{jiang2023mistral} without any fine-tuning to establish the zero-shot performance ceiling.
Second, we construct a LoRA fine-tuned baseline where we apply LoRA fine-tuning directly to Mistral-7B on the SQuAD dataset without any compression mechanism, thus representing the standard approach without context condensation.
This baseline operates without an encoder-decoder structure, functioning as a conventional LLM fine-tuned for question answering at full context length.
Third, we evaluate the ICAE model fine-tuned on PWC as provided by the original authors \cite{ge2023context}, which represents their reported best configuration.
Finally, we train our own ICAE variant by fine-tuning the pretrained encoder-decoder architecture on SQuAD using identical training code and hyperparameters to those employed by the authors for PWC fine-tuning.
This parallel setup enables direct comparison while controlling for implementation differences.

Table~\ref{tab:icae_squad} presents the evaluation results across all four configurations, measured using F1 scores on the SQuAD validation set.
It is important to note that we apply a hard rule to only compress texts if they are longer than 256 tokens, ensuring that short texts are never inflated by the fixed-length memory tokens.
Consequently, the compression ratio for ICAE variants averages approximately $1.7 \pm 0.7$, meaning contexts are condensed to roughly 60\% of their original length while maintaining the compressed representation.
\begin{table}[hbt]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} &
        \textbf{Compression} &
        \textbf{F1} \\
        \midrule
        \texttt{Baseline} (Mistral-7B)          & ×$1$         & 68 \\
        \texttt{Baseline+FT}                    & ×$1$         & \underline{65} \\
        \texttt{ICAE-PT+FT} (PwC, authors)      & ×$1.7\pm0.7$ & 57 \\
        \texttt{ICAE-PT+FT} (SQuAD, ours)       & ×$1.7\pm0.7$ & \textbf{73} \\
        \bottomrule
    \end{tabular}
    \caption{This table compares the performance of different models with and without compression. Mistral-7B is used as a base model for these experiments. The model names are explained in Section~\ref{sec:training-process-overview}. We also compare our Fine-Tuned model on SQuAD to the ICAE authors' Fine-Tuned model on PWC \cite{ge2023context}.}
    \label{tab:icae_squad}
\end{table}
The results in Table~\ref{tab:icae_squad}, using Mistral-7B, show that our SQuAD-finetuned ICAE model achieves a high F1 score of 73.
This surpasses not only the uncompressed baseline but also the uncompressed, LoRA fine-tuned baseline (73 vs. 65).
This outcome is notable, as compression typically implies information loss.
Here, it might be that the compression provides a beneficial inductive bias, helping the model focus on salient information.
However, the ICAE model fine-tuned on the PWC dataset used in the original ICAE work \cite{ge2023context} fails to generalize, underscoring the importance of in-domain fine-tuning.

For consistency across datasets, we also measure BLEU scores, as detailed in Section~\ref{sec:quality_metrics}.
Figure~\ref{fig:bleu_squad} presents these results for the Qwen3-8B model.
Here, the performance ordering differs: the LoRA fine-tuned baseline surpasses the fine-tuned ICAE model.
This is followed by the pretrained-only ICAE model, with the base model performing the lowest.
This suggests that while pretraining may temporarily diminish some of the model's capabilities, task-specific fine-tuning helps recover and enhance performance over the baseline on QA tasks.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.6\textwidth]{graphs/bleu/bleu_squad.pdf}
    \caption{BLEU scores on SQuAD using Qwen3-8B as the base model. 
    BLEU is calculated between the generated and the GT answer.}
    \label{fig:bleu_squad}
\end{figure}

The discrepancy in relative performance between Table~\ref{tab:icae_squad} and Figure~\ref{fig:bleu_squad} can be attributed to the different base models.
Despite their close parameter counts, Mistral-7B and Qwen3-8B exhibit distinct characteristics.
We hypothesize that Qwen3-8B is more responsive to LoRA fine-tuning, allowing the standard fine-tuned baseline to outperform the compressed variant.
This contrasts with the Mistral-7B results, where the ICAE compression provides a more significant relative benefit.

The above results indicate that ICAE compression might be viable for application to downstream tasks.
The strong performance on SQuAD motivates our subsequent investigation of the ICAE framework in more complex, agentic settings where context length presents significant computational challenges.



\section{ICAE Fine-Tuning and Results on Code Reconstruction (RepoQA)}
\label{sec:eval_repoqa}

To assess the fidelity of our compression mechanism on structured, technical data, we evaluated the ICAE model on the RepoQA benchmark~\cite{liu2024repoqa}.
This experiment serves as a test of the encoder's ability to preserve the high-fidelity, granular information inherent to source code, which is a prerequisite for any downstream software engineering task.
High-fidelity reconstruction is particularly critical for code, where even minor character-level differences (e.g., incorrect URLs, missing special characters) can lead to functional failures, as illustrated in Example~\ref{ex:ae-readme-recon}.
As detailed in Section~\ref{sec:datasets}, we use the "Searching Needle Function" task, which requires the model to reconstruct a specific target function (the "needle") from a long code context (the "haystack") based on a natural language description.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.43\textwidth}
        \includegraphics[width=\textwidth]{graphs/repoqa_val_loss.pdf}
        \caption{RepoQA Validation Loss}
        \label{fig:repoqa_val_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/bleu/bleu_repoqa.pdf}
        \caption{BLEU scores on RepoQA}
        \label{fig:bleu_repoqa}
    \end{subfigure}
    \caption{RepoQA evaluation metrics using Qwen3-8B as the base model. 
    The left graph shows the validation loss, which decreases steadily, indicating successful learning, but not even close to an uncompressed LoRA-FT (LLM-FT here refers to the no-compression \texttt{Baseline+FT}).
    The right graph displays BLEU scores for code reconstruction, where the LoRA fine-tuned baseline excels as well.
    }
    \label{fig:repoqa_metrics}
\end{figure}

We fine-tuned the LoRA weights of the encoder on this task, optimizing for token-level cross-entropy loss between the generated and ground-truth functions, exactly like we did for SQuAD.
The results of this experiment are summarized in Figure~\ref{fig:repoqa_metrics}.
Figure~\ref{fig:repoqa_val_loss} shows the validation loss during fine-tuning.
The loss curve displays a consistent downward trend, confirming that the model effectively learns to reconstruct the target functions from the compressed representations.
We also see that Baseline-LoRA instantly achieves better performance in terms of loss, which is expected.

Figure~\ref{fig:bleu_repoqa} presents the BLEU scores, which exhibit a performance ordering consistent with the trends observed in the SQuAD evaluation.
The LoRA fine-tuned baseline without compression achieves the highest reconstruction quality, followed by the ICAE model with both pretraining and fine-tuning (\texttt{ICAE-PT+FT}).
The uncompressed baseline without fine-tuning demonstrates intermediate performance, while the ICAE model with only pretraining (\texttt{ICAE-PT}) yields the lowest scores.
This hierarchy reinforces the pattern established in our earlier experiments (for all datasets combined, see Figure~\ref{fig:bleu_all_datasets}) and confirms that task-specific fine-tuning is essential for the compressed representations to approach the performance of uncompressed models.

In summary, the evaluation on RepoQA indicates that the ICAE framework is capable of compressing and reconstructing complex source code with a high degree of fidelity.

\section{ICAE Fine-Tuning and Results on SWE-bench Verified}

This section describes the main experiment of the thesis and discusses its results.
The conceptual methodology for applying ICAE to agentic trajectories is detailed in Chapter~\ref{cha:methodology}.
Figures~\ref{fig:icae-agent-training-overview} and~\ref{fig:icae-agent-training-step} in that chapter illustrate the training process, where the model is fine-tuned on agentic trajectories to predict tool calls from a history of compressed observations.
Here, we evaluate the performance of this method on the SWE-bench Verified dataset.

\subsection{Experimental Setup}
We reimplemented the ICAE framework from scratch, building upon the original architecture \cite{ge2023context} with several modifications for improved efficiency and reproducibility.
Our implementation uses the Qwen3 model family (specifically Qwen3-8B) as the base LLM, with LoRA adaptation applied to the attention matrices (q\_proj and v\_proj) using a rank of 128 (see all hyperparameters in Appendix~\ref{app:training_details}).
To reduce complexity, we explicitly turn off the model's "thinking" mode, as detailed in Appendix~\ref{sec:disabling_thinking}.
The pretraining and fine-tuning processes are described in more detail in Chapter~\ref{cha:methodology}.


\subsection{Evaluation and Results}
The results of our experiments are presented in Table~\ref{tab:qwen_icae_variants_absolute}. The configurations for the model variants, including details on the encoder and decoder setups, are described in Chapter~\ref{cha:methodology}. The table compares the models on two primary metrics:
\begin{itemize}
    \item \textbf{Resolved (/500)}: The number of issues successfully resolved out of 500.
    \item \textbf{Time (s)}: The mean time in seconds to generate a tool call.
\end{itemize}
For more details on how we trained the model variants, see Figure~\ref{fig:training-process-overview}.

\begin{table}[h]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.05}
  
    \begin{tabular}{|l|ll|cc|}
        \hline
        \textbf{Name in our paper} & \textbf{Encoder} & \textbf{Decoder} & \textbf{Resolved (/500) $\uparrow$} & \textbf{Time (s) $\downarrow$} \\
        \hline
        \multicolumn{5}{|l|}{\hspace{1em}\textit{Naive Baselines}} \\
        \hline
        —                         & del long obs-s            &     Qwen       & 1         & 0.44                      \\
        —                         & del all obs-s             &     Qwen       & 0         & 0.39                      \\
        \hline
        \multicolumn{5}{|l|}{\hspace{1em}\textit{Baselines}} \\
        \hline
        —                         & —                         & Qwen (Full-FT)   & \textbf{86}                       & 1.24                      \\
        Baseline+FT               & —                         & Qwen (LoRA-FT)   & 10      & 1.24                      \\
        Baseline                  & —                         & Qwen   & \underline{19.4 $\pm$ 6.5}                        & 1.23                      \\
        \hline
        \multicolumn{5}{|l|}{\hspace{1em}\textit{ICAE Compression (ours)}} \\
        \hline
        ICAE-PT                   & ICAE (LoRA-PT)         & Qwen           & 2                    & 1.23                      \\
        ICAE-PT+FT                & ICAE (LoRA-PT \& LoRA-FT)       & Qwen           & 7.8 $\pm$ 2.59                    & \textbf{1.12 (0.31+0.81)}    \\
        —                         & ICAE (LoRA-PT \& LoRA-FT)      & Qwen (LoRA-FT)   & 10          & \underline{1.13}              \\
        \hline
    \end{tabular}
    \caption{Performance on SWE-bench Verified.
    This table compares different model configurations on resolve rate and inference time.
    "Qwen" refers to the Qwen3-8B model without any fine-tuning.
    "del long/all obs-s" stands for deleting long (>256 tokens) or all observations during the inference process -- these are the naive baselines.
    PT stands for pretrained, and FT stands for fine-tuned.
    The fully fine-tuned Qwen model achieves the highest number of resolved issues, setting the upper bound for performance.
    While ICAE compression reduces inference time, it significantly lowers the task resolution rate compared to the uncompressed baseline.}
    \label{tab:qwen_icae_variants_absolute}
  \end{table}

We first establish two naive baselines to demonstrate the importance of retaining observation context.
The "del long obs-s" approach discards any observation exceeding 256 tokens, while "del all obs-s" removes all observations entirely.
As shown in Table~\ref{tab:qwen_icae_variants_absolute}, both methods result in a drastic drop in performance, with almost no issues resolved.
While they significantly reduce generation time by shortening the context, their failure highlights that observations are critical for task success, motivating the need for more sophisticated context management techniques like compression.

Next, we evaluate three uncompressed baseline models to set performance targets.
The fully fine-tuned Qwen3-8B model achieves the highest performance, resolving 86 issues and setting the upper bound for this architecture.
The LoRA fine-tuned variant ("\texttt{Baseline+FT}") provides a more parameter-efficient alternative, resolving 10 issues.
The base Qwen3-8B model without any fine-tuning ("\texttt{Baseline}") serves as the most direct point of comparison for our ICAE models, as they use this same frozen model as the decoder.
It resolves 19.4 $\pm$ 6.5 issues, establishing a solid baseline for an off-the-shelf model on this task.
The uncertainty ($\pm$ 6.5) represents the standard deviation across multiple evaluation runs (five runs for each model), quantifying the variability in the model's performance.

The core of our experiment tests ICAE with an encoder fine-tuned on SWE-bench trajectories.
When pairing the ICAE-FT encoder with the base Qwen decoder, we observe a modest 10\% reduction in generation time.
However, this configuration sees a significant drop in task performance, resolving only 7.8 issues compared to the baseline's 19.4.
A similar trend holds when using a LoRA-FT decoder, where the resolved rate plummets.
This suggests that while compression is time-efficient, it loses critical information necessary for end-to-end task success in this agentic setting.


\begin{figure}[hbt]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/bleu/bleu_swe-bench.pdf}
      \caption{BLEU scores on SWE-bench}
      \label{fig:bleu_swe-bench}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.40\textwidth}
      \includegraphics[width=\textwidth]{graphs/boxplot-main.pdf}
      \caption{Resolve rate comparison}
      \label{fig:boxplot}
  \end{subfigure}
  \caption{BLEU scores and box plot for SWE-bench Verified. The left chart shows that the fine-tuned ICAE model achieves a higher BLEU score than the baseline. However, the right box plot reveals that the baseline model resolves significantly more issues. The comparison is based on 5 evaluations of the models. Each box represents the interquartile range of the resolve rate, and the triangle stands for the mean value.}
  \label{fig:bleu-boxplot-combined}
\end{figure}


The primary negative finding of this study is the substantial decrease in task resolution performance when using ICAE compression.
As shown in Table~\ref{tab:qwen_icae_variants_absolute}, the \texttt{ICAE-PT+FT} configuration resolved only 7.8 $\pm$ 2.59 issues, a marked decline from the 19.4 $\pm$ 6.5 issues resolved by the uncompressed baseline.
A two-sample Welch t-test on the number of resolved issues across five runs confirms this difference: the baseline achieved a mean of 19.4 resolved issues ($s = 6.54$), while the ICAE model achieved 7.8 ($s = 2.59$), yielding $t(\text{DoF} = 5.22) = 3.69$, $p = 0.013 < \alpha = 0.05$.
This establishes that the compressed model on average underperforms the baseline in the agentic setting (also note Figure~\ref{fig:boxplot})

This outcome presents a discrepancy when compared to offline evaluation metrics.
The BLEU scores for predicting the next tool call, shown in Figure~\ref{fig:bleu_swe-bench}, follow a trend consistent with prior experiments on SQuAD and RepoQA, where the \texttt{ICAE-PT+FT} model outperforms the uncompressed baseline.
This highlights a critical distinction: BLEU is calculated on static, pre-recorded trajectories (by a smarter teacher model, so we take them as ground truth answers), whereas the number of resolved issues is an online metric derived from the agent's autonomous interaction with the environment.
We discuss why this might be the case in more detail in Section~\ref{cha:discussion}.

\paragraph{Additional experimental configurations.}
We also investigated additional experimental configurations.
We hypothesized that training both the encoder and the decoder simultaneously would improve performance since the number of trained parameters would increase in that case.
In one variant, the LoRA-adapted decoder was unfrozen during the fine-tuning stage, allowing for simultaneous training of both the ICAE encoder and the decoder.
As reported in Table~\ref{tab:qwen_icae_variants_absolute}, this approach (LoRA-PT \& LoRA-FT encoder with LoRA+FT decoder) did not yield a notable performance improvement relative to the uncompressed LoRA-finetuned baseline.

Furthermore, an interesting observation was made regarding the \texttt{Baseline+FT} model.
Within the online agentic setting of SWE-bench, this configuration substantially underperformed the non-finetuned \texttt{Baseline} model.
This outcome contrasts with its performance on offline benchmarks such as RepoQA, where LoRA fine-tuning resulted in a significant performance gain (Figure~\ref{fig:bleu_repoqa}).
This signals that probably the efficacy of LoRA fine-tuning is task-dependent and its benefits do not necessarily generalize from static benchmarks to dynamic, interactive settings.
For more details, see Chapter~\ref{cha:discussion} (\Cref{sec:lora_bottleneck}).


\subsection{Impact on Inference Efficiency}
\label{sec:rq1_efficiency}

In this section, we address \hyperref[rq:1]{RQ1}, investigating how implicit context condensation influences the efficiency of LLM-based agents.
We measure efficiency in terms of two key metrics: trajectory length (the number of interaction steps within the context window) and inference time (the average time of generating a tool call).

\paragraph{Trajectory Length.}
Firstly, we analyze the number of interaction steps each agent could perform before termination. For this analysis, the default 75-turn hard cap was disabled, and termination occurred when the context window limit of 32,768 tokens was reached.
Our findings provide a clear affirmative answer regarding the benefits of compression.
The \texttt{ICAE-PT+FT} model was able to execute significantly longer trajectories compared to the uncompressed baseline.
On average, the compressed agent performed 113 steps before termination, a 40\% increase over the baseline's average of 81 steps.
This demonstrates that by condensing lengthy observations into a fixed number of memory tokens, our approach effectively creates more space within the context window, enabling the agent to engage in more extensive problem-solving dialogues.
 
The difference in trajectory length is further illustrated by the box plot in Figure~\ref{fig:boxplot-stepcount}.
The plot compares the distribution of step counts at termination for the baseline agent and the ICAE-compressed agent.
The mean step count for the ICAE agent is visibly higher than that of the baseline, and the entire interquartile range is shifted upwards.
Although both models exhibit outliers representing exceptionally long trajectories, the overall distribution for the ICAE agent is skewed towards a higher number of steps, reinforcing the conclusion that compression enables more prolonged interactions.


\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.65\textwidth]{graphs/boxplot-stepcount.pdf}
    \caption{Comparison of trajectory lengths at termination. This box plot shows that the ICAE-based agent (right) executes a significantly higher number of steps before reaching the context limit, compared to the uncompressed baseline agent (left). The context window limit is 32,768 tokens for this experiment, while there is no limit on the maximum number of steps the agent can take.}
    \label{fig:boxplot-stepcount}
\end{figure}

\paragraph{Inference Time.}
Secondly, we analyze the computational efficiency of the ICAE compression approach in terms of generation time during inference.
The quantitative results, presented in Table~\ref{tab:qwen_icae_variants_absolute}, indicate a measurable reduction in the time required for tool call generation.
Specifically, the compressed agent achieved a mean generation time of 1.12 seconds per tool call, representing an approximate 10\% reduction compared to the uncompressed baseline's mean of 1.23 seconds.
This total time for the compressed agent is composed of two distinct operational phases: the compression of the observation, which averages 0.31 seconds, and the subsequent generation of the next tool call, which averages 0.81 seconds.
The observed reduction in generation time is a direct consequence of the decreased context size processed by the decoder following the compression step.

It is relevant to note that these timing measurements were obtained in an experimental setup that did not utilize KV-caching.
The absence of KV-caching implies that the reported speedups represent conservative estimates of the potential efficiency gains, as the baseline model processes the full, growing context at every step.
A more detailed technical discussion regarding the absence of KV-caching and its implications for real-world deployment scenarios is provided in Section~\ref{sec:limitations}.
These findings confirm that implicit context condensation positively influences inference efficiency, reducing the time per step in the agentic workflow.




\subsection{Discussion of Performance on Agentic Tasks versus Standard NLP Tasks}
\label{sec:rq2_performance_domains}


\begin{figure}[hbt]
    \centering
    \includegraphics[width=1.0\textwidth]{graphs/bleu/bleu_all_datasets.pdf}
    \caption{BLEU scores comparison across all datasets. This chart consistently shows that the fine-tuned ICAE model (\texttt{ICAE-PT+FT}) outperforms the uncompressed baseline in terms of BLEU score.
    It is also easy to see that the trend is the same for all datasets.}
    \label{fig:bleu_all_datasets}
\end{figure}
  

In this section, we address \hyperref[rq:2]{RQ2}, examining whether the performance of implicit context condensation on standard NLP benchmarks transfers to the more complex domain of agentic software engineering.
Our findings indicate that the strong performance of ICAE on standard NLP benchmarks does not directly translate to the dynamic, multi-step environment of agentic software engineering tasks.
While offline metrics suggested promise, the practical application revealed a significant performance degradation.
As illustrated in Figure~\ref{fig:boxplot}, the \texttt{ICAE-PT+FT} model is statistically significantly outperformed by the uncompressed baseline model in terms of successfully resolved issues.
This outcome highlights the gap between single-step, static evaluations and the complexities of online, interactive problem-solving.
Interestingly, token-level metrics paint a different picture: Figure~\ref{fig:bleu_all_datasets} shows that the fine-tuned ICAE model (\texttt{ICAE-PT+FT}) achieves a higher BLEU score than the uncompressed baseline across all datasets.
This suggests that while the model learns to mimic the expert's next action well on a static dataset, this capability does not translate to effective problem-solving when the agent must navigate the consequences of its own actions in a live environment.
