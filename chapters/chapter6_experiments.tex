% ========================================
% CHAPTER 6: EXPERIMENTS AND EVALUATION (ACTUAL SETUPS AND RESULTS)
% ========================================
\chapter{Experiments}
\label{cha:experiments}

%
\section{Feasibility of Training-Free Context Condensation}

We first investigate whether meaningful context compression can be achieved without any model training.
These initial experiments explore methods for replacing discrete tokens with continuous representations, testing the hypothesis that simple embedding aggregation can preserve essential information for downstream tasks.

We explore two primary settings for training-free condensation.
In the \textbf{hard embedding} setting, discrete tokens are represented as one-hot vectors that index the input embedding matrix.
For condensation, we compute the elementwise mean of these vectors.
In the \textbf{soft-embedding} setting, we bypass the argmax operation and token lookup, feeding a continuous mixture of embeddings directly to the model.
Figure~\ref{fig:ser1} illustrates the standard token processing pathway, while Figure~\ref{fig:ser2} depicts our modification, which injects these continuous embeddings directly.

\paragraph{Online soft-embedding}
The online pathway is implemented by bypassing token sampling and the embedding lookup.
After running a standard decoding step to obtain logits, we compute the corresponding expected embedding and insert this continuous vector directly as the input for the next step using KV-cache manipulation.
This approach is illustrated in Figure~\ref{fig:ser2}.
The goal is to assess whether context can be compressed without trained adapters.

\paragraph{Regenerate-LLM offline}
To mitigate potential collapse from iterative generation, we also explored an offline method.
In this approach, we prompt the model to reproduce a given input sequence under teacher forcing and record all output embeddings at each step.
At inference time, these saved embeddings are reused as the context representation, avoiding online recomputation.

\paragraph{Results}
Table~\ref{tab:avg_variants} reports SQuAD performance under these condensation strategies.
The results establish that replacing discrete tokens with untrained condensed mixtures, whether generated online or offline, substantially degrades performance on the QA task.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Setting (SQuAD), context embed} &
        \textbf{Exact Match} & \textbf{F1} \\
        \midrule
        Baseline — hard tokens         & \textbf{0.58} & \textbf{0.71} \\
        Hard embedded, avg ×2          & 0.09 & 0.21 \\
        Soft embedded online, avg ×2          & 0.05 & 0.11 \\
        Soft embedded \text{Regenerate-LLM}, avg ×2          & 0.07 & 0.16 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different training-free context condensation methods}
    \label{tab:avg_variants}
\end{table}

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/ser1.jpeg}
        \caption{Original approach}
        \label{fig:ser1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/ser2.jpeg}
        \caption{Our modification variant (soft embedding, online)}
        \label{fig:ser2}
    \end{subfigure}
    \caption{Visualization of the "without training" approach.}
    \label{fig:ser_combined}
\end{figure}


\section{Learning Projections for Context Condensation}

We as well investigate whether a trained, low-capacity projection module could learn to compress adjacent embedding vectors while preserving task-relevant information.
We explore several architectural variants to map a pair of concatenated hidden vectors $[e_{2t-1}; e_{2t}]$ from $\mathbb{R}^{2d}$ to a single vector in $\mathbb{R}^{d}$.
These variants included a simple \textbf{linear projector}, a shallow non-linear \textbf{MLP} (one to two layers with GELU activation), and a full \textbf{BERT encoder} \cite{devlin2018bert} to provide a higher-capacity compression mechanism.

All experiments used the SQuAD context-embedding setting, with the base Qwen3-8B model frozen.
Only the parameters of the projection module were trained via token-level cross-entropy on the answer generation task.
Across all architectural variants, the models failed to generalize.
As shown in Figure~\ref{fig:squad_metrics}, while the models were able to overfit to the training data, the validation loss flattened after an initial small improvement.
The resulting F1 scores remained significantly below the uncompressed baseline, demonstrating that these simple projection methods could not recover the performance lost to compression.

We also explored several modifications to the training protocol through systematic ablation studies.
We varied the projection type (linear versus 1- or 2-layer MLP), normalization schemes (pre/post LayerNorm, scale-preserving residual gates), regularization techniques (weight decay, dropout), and the decision to re-project via vocabulary space versus staying in hidden space.
Additionally, we experimented with unfreezing the token embedding table while keeping the main transformer blocks frozen.
None of these changes meaningfully altered the outcome; the models were consistently unable to train and generalize to the SQuAD task.

The failure of these methods suggests two potential underlying issues.
First, the embedding manifold may possess a non-smooth or complex geometric structure that is disrupted by simple linear or shallow non-linear projections, leading to irreversible information loss that the frozen decoder cannot overcome.
Second, even the higher-capacity BERT encoder may lack the expressive power to learn a sufficiently meaning-preserving compression when paired with a much larger, frozen decoder.
\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{graphs/squad_val_loss.pdf}
        \caption{}
        \label{fig:squad_val_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{graphs/squad_f1_score.pdf}
        \caption{}
        \label{fig:squad_f1_score}
    \end{subfigure}
    \caption{SQuAD validation metrics for projection-based compression.}
    \label{fig:squad_metrics}
\end{figure}

\section{ICAE Pretraining and Results on General Text Reconstruction}
\label{sec:icae_pretraining_results}

This section details the pretraining phase of our In-Context Autoencoder, hereafter referred to as \texttt{ICAE-PT}.
The methodology for this pretraining stage is conceptually outlined in Chapter~\ref{cha:methods}.
The primary objective of this phase is to train the encoder to generate compressed representations from which the original text can be accurately reconstructed.


The pretraining stage was performed on the dataset SlimPajama-6B \cite{together2023redpajama}\footnote{\url{https://huggingface.co/datasets/DKYoon/SlimPajama-6B}} using a combination of autoencoding and language modeling objectives.
It is a common text dataset, that is used for pretraining LLMs, consisting of 6 billion tokens (a random 1\% of the original 627B tokens).
The dataset that authors used in the ICAE paper \cite{ge_-context_2024} "The Pile" is unavailable.
Following the original ICAE methodology, the training objective was a 50/50 mix of an autoencoding (AE) task, signaled by a special \texttt{<AE>} token, and a language modeling (LM) task, which used no special token.
The AE process is conceptually illustrated in Figure~\ref{fig:icae}.

On Figure~\ref{fig:loss_final} you can notice the sudden drop of the loss.
This is a phenomenon found in \cite{zhao_position_2025}.
It appears due to the modification of positional encodings for the memory tokens.
We see the effect being the same as described by the authors.
In our experiments, it only appears if we apply the Position ID manipulation.


\begin{figure}[hbt]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/pretraining_loss_final.pdf}
      \caption{}
      \label{fig:loss_final}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/pretraining_bleu_final.pdf}
      \caption{}
      \label{fig:bleu_final}
  \end{subfigure}
  \caption{Pre-training evaluation metrics across different model configurations.}
  \label{fig:pretraining-metrics-combined}
\end{figure}

\begin{table}[h]
    \centering
    \small
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{lll c}
        \toprule
        \textbf{Run} & \textbf{Checkpoint (\# steps)} & \textbf{Compression} & \textbf{BLEU (mean, n=100)} \\
        \midrule
        Qwen3-8B/full (no ICAE) & 18k & $\times 1$ & 0.867 \\
        \hline
        \multicolumn{4}{|l|}{\hspace{1em}\textit{10M tokens subset}} \\
        \hline
        \texttt{ICAE-PT} & 9k & $\times 4$ & 0.909 \\
        \texttt{ICAE-PT} & 12k & $\times 4$ & \underline{0.942} \\
        \texttt{ICAE-PT} & 18k & $\times 4$ & 0.902 \\
        \hline
        \multicolumn{4}{|l|}{\hspace{1em}\textit{1B tokens subset}} \\
        \hline
        \texttt{ICAE-PT} & 9k  & $\times 4$ & 0.936 \\
        \texttt{ICAE-PT} & 12k (main) & $\times 4$ & \textbf{0.964} \\
        \texttt{ICAE-PT} & 18k & $\times 4$ & 0.928 \\
        \bottomrule
    \end{tabular}
    \caption{Autoencoding (AE) reconstruction BLEU on SQuAD contexts (100-sample evaluations only). The 1B tokens subset 12k checkpoint is the main model used for fine-tuning.}
    \label{tab:ae_bleu_squad_ours}
\end{table}

We evaluate \texttt{ICAE-PT} autoencoding (AE) pretraining using Qwen3-8B as the base model.
During AE, the encoder compresses input contexts at a fixed $\times 4$ ratio (specifically, $1024\!\to\!256$ tokens on average), and the decoder reconstructs the original text.
We report BLEU on SQuAD contexts tested on 100 samples.
The checkpoint using 1B tokens for 12k steps is selected as the main run and used for fine-tuning.

We have also experimented with compressing 16 tokens into 4 on average instead of 1024 into 256.
It which achieved a higher BLEU (\(\approx 0.982\)). 
Notably, none of the 100-sample AE scores reach \(\approx 0.99\).
This may be acceptable for general text but could be material for code, where near-lossless reconstruction is likely a prerequisite for downstream stability.

\medskip
\noindent\textit{Example} 
Below we show a short example, where we tried to reconstruct the README.md file of the SWE-agent project.
The difference is highlighted in yellow.

\noindent\textbf{Original:}

\quad\texttt{<p align="center">}

\quad\texttt{<a href="https://swe-agent.com/latest/">}

\quad\texttt{<strong>Documentation</strong></a>\&nbsp;}

\quad\texttt{...}

\noindent\textbf{Reconstructed:}

\quad\texttt{<p align="center">}

\quad\texttt{<a href="https://swe-agent.com}\colorbox{yellow}{\texttt{/agent/}}\texttt{latest/">}

\quad\texttt{<strong>Documentation</strong></a>\&nbsp;}

\quad\texttt{...}

\noindent Even in the very start of the text, the difference is noticeable: the hallucinated \texttt{/agent/} path segment in the URL, which could break navigation in a coding task.


In line with internal feedback, these AE findings suggest that the current pretrain/fine-tuning mixes undertrain the model on code: AE BLEU for code should approach text-level (near 1.0) to avoid even small inaccuracies (e.g., link/variable name substitutions).


% ========================================
% SECTION 5.3: EVALUATION ON QUESTION ANSWERING TASKS (OFFLINE)
% ========================================
\section{ICAE Fine-Tuning and Results on Question Answering}

While the original ICAE work \cite{ge_-context_2024} demonstrated promising results on their proprietary PWC dataset, we sought to validate these findings on a well-established benchmark to ensure generalizability and facilitate fair comparison with existing approaches.
To address the limitation of the authors'-crafted PWC dataset, we conducted fine-tuning experiments on the Stanford Question Answering Dataset (SQuAD) \cite{rajpurkar2016squad}, a widely recognized benchmark in the question answering literature that provides standardized evaluation protocols and enables reproducible comparisons.

During fine-tuning on SQuAD, the encoder compresses the context while the question remains uncompressed (text-compressed-text format).
The decoder generates answers from this mixed representation.
We use identical LoRA hyperparameters as in \cite{ge_-context_2024}.

To establish a comprehensive evaluation baseline, we compare four distinct model configurations that systematically vary the training procedure and compression strategy.
First, we evaluate the base Mistral-7B model \cite{jiang2023mistral} without any fine-tuning to establish the zero-shot performance ceiling.
Second, we construct a LoRA fine-tuned baseline where we apply LoRA fine-tuning directly to Mistral-7B on the SQuAD dataset without any compression mechanism, thus representing the standard approach without context condensation.
This baseline operates without an encoder-decoder structure, functioning as a conventional LLM fine-tuned for question answering at full context length.
Third, we evaluate the ICAE model fine-tuned on PWC as provided by the original authors \cite{ge_-context_2024}, which represents their reported best configuration.
Finally, we train our own ICAE variant by fine-tuning the pretrained encoder-decoder architecture on SQuAD using identical training code and hyperparameters to those employed by the authors for PWC fine-tuning.
This parallel setup enables direct comparison while controlling for implementation differences.

Table~\ref{tab:icae_squad} presents the evaluation results across all four configurations, measured using F1 scores on the SQuAD validation set.
The compression ratio for ICAE variants averages approximately $1.7 \pm 0.7$, meaning contexts are condensed to roughly 60\% of their original length while maintaining the compressed representation.
\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tabular}{lcc}
            \toprule
            \textbf{Model} &
            \textbf{Compression} &
            \textbf{F1} \\
            \midrule
            \texttt{Baseline} (Mistral-7B)          & ×$1$         & 68 \\
            \texttt{Baseline+FT}                    & ×$1$         & \underline{65} \\
            \texttt{ICAE-PT+FT} (PwC, authors)      & ×$1.7\pm0.7$ & 57 \\
            \texttt{ICAE-PT+FT} (SQuAD, ours)       & ×$1.7\pm0.7$ & \textbf{73} \\
            \bottomrule
        \end{tabular}
        \caption{F1 scores on SQuAD using Mistral-7B as the base model}
        \label{tab:icae_squad}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/bleu_squad.pdf}
        \caption{BLEU scores on SQuAD using Qwen3-8B as the base model}
        \label{fig:bleu_squad}
    \end{subfigure}
    \caption{SQuAD evaluation results}
    \label{fig:squad_results}
\end{figure}

The results in Table~\ref{tab:icae_squad}, using Mistral-7B, show that our SQuAD-finetuned ICAE model achieves a high F1 score of 73.
This surpasses not only the uncompressed baseline but also the uncompressed, LoRA fine-tuned baseline (73 vs. 65).
This outcome is notable, as compression typically implies information loss;
here, it appears to provide a beneficial inductive bias, helping the model focus on salient information.
However, the ICAE model fine-tuned on the proprietary PWC dataset fails to generalize, underscoring the importance of in-domain fine-tuning.

For consistency across datasets, we also measure BLEU scores, as detailed in Section~\ref{sec:quality_metrics}.
Figure~\ref{fig:bleu_squad} presents these results for the Qwen3-8B model.
Here, the performance ordering differs: the LoRA fine-tuned baseline surpasses the fine-tuned ICAE model.
This is followed by the pretrained-only ICAE model, with the base model performing lowest.
This suggests that while pretraining may temporarily diminish some of the model's capabilities, task-specific fine-tuning helps recover and enhance performance over the baseline on QA tasks.

The discrepancy in relative performance between Table~\ref{tab:icae_squad} and Figure~\ref{fig:bleu_squad} can be attributed to the different base models.
Despite their close parameter counts, Mistral-7B and Qwen3-8B exhibit distinct characteristics.
We hypothesize that Qwen3-8B is more responsive to LoRA fine-tuning, allowing the standard fine-tuned baseline to outperform the compressed variant.
This contrasts with the Mistral-7B results, where the ICAE compression provides a more significant relative benefit.

These results provide encouraging evidence for the viability of applying ICAE-based compression to downstream tasks.
The strong performance on SQuAD, combined with the observed compression benefits, motivated our subsequent investigation of the ICAE framework in more complex, agentic settings where context length presents significant computational challenges.



\section{ICAE Fine-Tuning and Results on Code Reconstruction}
\label{sec:eval_repoqa}

To assess the fidelity of our compression mechanism on structured, technical data, we evaluated the ICAE model on the RepoQA benchmark~\cite{liu2024repoqa}.
This experiment serves as a test of the encoder's ability to preserve the high-fidelity, granular information inherent to source code, which is a prerequisite for any downstream software engineering task.
As detailed in Section~\ref{sec:datasets}, we use the "Searching Needle Function" task, which requires the model to reconstruct a specific target function (the "needle") from a long code context (the "haystack") based on a natural language description.

We fine-tuned the LoRA weights of the encoder on this task, optimizing for token-level cross-entropy loss between the generated and ground-truth functions, exactly like we did for SQuAD.

The results of this experiment are summarized in Figure~\ref{fig:repoqa_metrics}.
Figure~\ref{fig:repoqa_val_loss} shows the validation loss during fine-tuning.
The loss curve displays a consistent downward trend, confirming that the model effectively learns to reconstruct the target functions from the compressed representations.
We also see that Basline-LoRA instantly achieves better performance in terms of loss, which is expected.

Figure~\ref{fig:bleu_repoqa} presents the BLEU scores, which exhibit a performance ordering consistent with the trends observed in the SQuAD evaluation.
The LoRA fine-tuned baseline without compression achieves the highest reconstruction quality, followed by the ICAE model with both pretraining and fine-tuning (\texttt{ICAE-PT+FT}).
The uncompressed baseline without fine-tuning demonstrates intermediate performance, while the ICAE model with only pretraining (\texttt{ICAE-PT}) yields the lowest scores.
This hierarchy reinforces the pattern established in our earlier experiments and confirms that task-specific fine-tuning is essential for the compressed representations to approach the performance of uncompressed models.

In summary, the evaluation on RepoQA indicates that the ICAE framework is capable of compressing and reconstructing complex source code with a high degree of fidelity.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/repoqa_val_loss.pdf}
        \caption{RepoQA Validation Loss}
        \label{fig:repoqa_val_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{graphs/bleu_repoqa.pdf}
        \caption{BLEU scores on RepoQA}
        \label{fig:bleu_repoqa}
    \end{subfigure}
    \caption{RepoQA evaluation metrics.}
    \label{fig:repoqa_metrics}
\end{figure}

\section{ICAE Fine-Tuning and Evaluation on SWE-bench Verified}

This section describes the main experiment of the thesis and discusses its results.
The conceptual methodology for applying ICAE to agentic trajectories is detailed in Chapter~\ref{cha:methods}.
Figures~\ref{fig:icae-agent-training-overview} and~\ref{fig:icae-agent-training-step} in that chapter illustrate the training process, where the model is fine-tuned on expert trajectories to predict tool calls from a history of compressed observations.
Here, we evaluate the performance of this method on the SWE-bench Verified dataset.

\subsection{Experimental Setup}
We reimplemented the ICAE framework from scratch, building upon the original architecture \cite{ge_-context_2024} with several modifications for improved efficiency and reproducibility.
Our implementation uses the Qwen3 model family (specificaly Qwen3-8B) as the base LLM, with LoRA adaptation applied to the attention matrices (q\_proj and v\_proj) using a rank of 128 (see all hyperparameters in Appendix~\ref{app:training_details}).
To make things simpler, we explicitly disable "thinking", more on that can be found in Appendix~\ref{sec:disabling_thinking}.
The pretraining and fine-tuning processes are described in more detail in Chapter~\ref{cha:methods}.


\subsection{Results}
The results of our experiments are presented in Table~\ref{tab:qwen_icae_variants_absolute}. The table compares several model configurations across five oder is only applied if akey metrics:
\begin{itemize}
    \item \textbf{Encoder}: The model used to compress observations. "—" indicates no encoder and thus no compression.
    \item \textbf{Decoder}: The model used to generate the next tool call. We test the base Qwen3-8B ("\texttt{Baseline}"), a LoRA fine-tuned version ("Baseline-LoRA+FT"), and a fully fine-tuned version ("Baseline-Full+FT")
    \item \textbf{Resolved (/500)}: The number of issues successfully resolved out of 500.
    \item \textbf{Time (s)}: The mean time in seconds to generate a tool call.
\end{itemize}

For more details on the model variants, see Figure~\ref{fig:training-process-overview}.

We first establish two naive baselines to demonstrate the importance of retaining observation context.
The "del long obs-s" approach discards any observation exceeding 256 tokens, while "del all obs-s" removes all observations entirely.
As shown in Table~\ref{tab:qwen_icae_variants_absolute}, both methods result in a drastic drop in performance, with almost no issues resolved.
While they significantly reduce generation time by shortening the context, their failure highlights that observations are critical for task success, motivating the need for more sophisticated context management techniques like compression.

Next, we evaluate three uncompressed baseline models to set performance targets.
The fully fine-tuned Qwen3-8B model ("Baseline+Full-FT") achieves the highest performance, resolving 86 issues and setting the upper bound for this architecture.
The LoRA fine-tuned variant ("Baseline+LoRA-FT") provides a more parameter-efficient alternative, resolving 10 issues.
The base Qwen3-8B model without any fine-tuning ("\texttt{Baseline}") serves as the most direct point of comparison for our ICAE models, as they use this same frozen model as the decoder.
It resolves 19.4 $\pm$ 6.5 issues, establishing a solid baseline for an off-the-shelf model on this task.

%

\begin{table}[h]
  \centering
  \small
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.05}

  \begin{tabular}{|ll|cc|}
      \hline
      \textbf{Encoder} & \textbf{Decoder} & \textbf{Resolved (/500) $\uparrow$} & \textbf{Time (s) $\downarrow$} \\
      \hline
      \multicolumn{4}{|l|}{\hspace{1em}\textit{Naive Baselines}} \\
      \hline
      del long obs-s            &     Baseline       & 1         & 0.44                      \\
      del all obs-s             &     Baseline       & 0         & 0.39                      \\
      \hline
      \multicolumn{4}{|l|}{\hspace{1em}\textit{Baselines}} \\
      \hline
      —                         & Baseline+Full-FT   & \textbf{86}                       & 1.24                      \\
      —                         & Baseline+LoRA-FT   & 10 $\pm$ ?      & 1.24                      \\
      —                         & Baseline      & \underline{19.4 $\pm$ 6.5}                        & 1.23                      \\
      \hline
      \multicolumn{4}{|l|}{\hspace{1em}\textit{ICAE Compression (ours)}} \\
      \hline
      Baseline-PT+FT (ICAE)      & Baseline           & 7.8 $\pm$ 2.59                    & \textbf{1.12 (0.31+0.81)}    \\
      Baseline-PT+FT (ICAE)      & Baseline+LoRA-FT   & 10          & \underline{1.13}              \\
      \hline
  \end{tabular}
  \caption{Performance comparison of different model configurations on SWE-bench Verified.}
  \label{tab:qwen_icae_variants_absolute}
\end{table}



\begin{figure}[hbt]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/bleu_swe-bench_trajectories_ar.pdf}
      \caption{BLEU scores on SWE-bench}
      \label{fig:bleu_swe-bench}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{graphs/boxplot-main.png}
      \caption{Box plot comparison of the baseline against the \texttt{ICAE-PT+FT} model}
      \label{fig:boxplot}
  \end{subfigure}
  \caption{BLEU scores and box plot comparison of baseline and ICAE-compressed models.}
  \label{fig:bleu-boxplot-combined}
\end{figure}


The core of our experiment tests ICAE with an encoder fine-tuned on SWE-bench trajectories.
When pairing the ICAE-FT encoder with the base Qwen decoder, we observe a modest 10\% reduction in generation time.
However, this configuration sees a significant drop in task performance, resolving only 7.8 issues compared to the baseline's 19.4.
A similar trend holds when using a LoRA-FT decoder, where the resolved rate plummets.
This suggests that while compression is efficient, it loses critical information necessary for end-to-end task success in this agentic setting.

The primary negative finding of this study is the substantial decrease in task resolution performance when using ICAE compression.
As shown in Table~\ref{tab:qwen_icae_variants_absolute}, the \texttt{ICAE-PT+FT} configuration resolved only 7.8 $\pm$ 2.59 issues, a marked decline from the 19.4 $\pm$ 6.5 issues resolved by the uncompressed baseline.
The non-overlapping standard deviation intervals indicate that this performance gap is statistically significant, establishing that the compressed model underperforms the baseline in the agentic setting (Figure~\ref{fig:boxplot})

This outcome presents a discrepancy when compared to offline evaluation metrics.
The BLEU scores for predicting the next tool call, shown in Figure~\ref{fig:bleu_swe-bench}, follow a trend consistent with prior experiments on SQuAD and RepoQA, where the \texttt{ICAE-PT+FT} model outperforms the uncompressed baseline.
This highlights a critical distinction: BLEU is calculated on static, pre-recorded expert trajectories, whereas the number of resolved issues is an online metric derived from the agent's autonomous interaction with the environment.
In the latter, the agent's actions directly influence subsequent observations.

\paragraph{Additional experimental configurations.}
We also investigated additional experimental configurations.
In one variant, the LoRA-adapted decoder was unfrozen during the fine-tuning stage, allowing for simultaneous training of both the ICAE encoder and the decoder.
As reported in Table~\ref{tab:qwen_icae_variants_absolute}, this approach (\texttt{ICAE-PT+FT} encoder with \texttt{Baseline+LoRA-FT} decoder) did not yield a notable performance improvement relative to the uncompressed LoRA-finetuned baseline.

Furthermore, an interesting observation was made regarding the \texttt{Baseline+LoRA-FT} model.
Within the online agentic setting of SWE-bench, this configuration substantially underperformed the non-finetuned \texttt{Baseline} model.
This outcome contrasts with its performance on offline benchmarks such as RepoQA, where LoRA fine-tuning resulted in a significant performance gain (Figure~\ref{fig:bleu_repoqa}).
This indicates that the efficacy of LoRA fine-tuning is task-dependent and its benefits do not necessarily generalize from static benchmarks to dynamic, interactive settings.

\paragraph{Efficiency Gains.}
Beyond trajectory length, we also examined the computational efficiency of the ICAE compression approach.
The compressed agent demonstrated measurable improvements in generation speed, achieving approximately 10\% faster mean tool-call generation time compared to the uncompressed baseline (1.12s versus 1.23s per tool call), as shown in Table~\ref{tab:qwen_icae_variants_absolute}.
This speedup is composed of two phases: the compression of the observation (averaging 0.31s) and the subsequent generation of the next tool call (averaging 0.81s).
The reduction in generation time is a direct consequence of the smaller context size that the decoder must process after compression.

It is important to note that these timing measurements were obtained without the use of KV-caching, which would typically accelerate generation in multi-turn scenarios.
The absence of KV-caching in our experimental setup means that the reported speedups are conservative estimates of the potential efficiency gains.
A more detailed discussion of this limitation and its implications for real-world deployment is provided in Section~\ref{sec:limitations}.


\subsection{Impact on Agentic Trajectory Length}
\label{sec:rq1_trajectory_length}

To address our first research question, we analyzed the number of interaction steps each agent could perform before reaching the context window limit of 32,768 tokens (the maximum context length of Qwen3-8B).
Our findings provide a clear affirmative answer.
The \texttt{ICAE-PT+FT} model, which employs compression, was able to execute significantly longer trajectories compared to the uncompressed baseline.
On average, the compressed agent performed 113 steps before termination, a 40\% increase over the baseline's average of 81 steps.
This demonstrates that by condensing lengthy observations, our approach effectively creates more space within the context window, enabling the agent to engage in more extensive problem-solving dialogues.

The difference in trajectory length is further illustrated by the box plot in Figure~\ref{fig:boxplot-stepcount}.
The plot compares the distribution of step counts at termination for the baseline agent and the ICAE-compressed agent.
The mean step count for the ICAE agent is visibly higher than that of the baseline, and the entire interquartile range is shifted upwards.
Although both models exhibit outliers representing exceptionally long trajectories, the overall distribution for the ICAE agent is skewed towards a higher number of steps, reinforcing the conclusion that compression enables more prolonged interactions.

This result is a direct consequence of the ICAE mechanism.
The baseline model must store all the observations in its context, which quickly consumes the available token budget.
In contrast, the ICAE model compresses these observations into a fixed number of memory tokens, substantially reducing their footprint.
This efficiency gain allows the agent to accumulate a much larger history of interactions before exhausting its context, thereby theoretically allowing it to successfully complete the task.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.6\textwidth]{graphs/boxplot-stepcount.jpg}
    \caption{Comparison of the number of steps at termination for the baseline model and the ICAE-compressed model}
    \label{fig:boxplot-stepcount}
\end{figure}




\subsection{Discussion of Performance on Agentic Tasks versus Standad NLP Tasks}
\label{sec:rq2_performance_domains}


\begin{figure}[hbt]
    \centering
    \includegraphics[width=1.0\textwidth]{graphs/bleu_all_datasets.pdf}
    \caption{BLEU scores comparison across all datasets}
    \label{fig:bleu_all_datasets}
\end{figure}
  

Our findings indicate that the strong performance of ICAE on standard NLP benchmarks does not directly translate to the dynamic, multi-step environment of agentic software engineering tasks.
While offline metrics suggested promise, the practical application revealed a significant performance degradation.
As illustrated in Figure~\ref{fig:boxplot}, the \texttt{ICAE-PT+FT} model is statistically significantly outperformed by the uncompressed baseline model in terms of successfully resolved issues.
This outcome highlights the gap between single-step, static evaluations and the complexities of online, interactive problem-solving.

This trend is also visible in token-level metrics like BLEU.
Figure~\ref{fig:bleu_all_datasets} shows that for all datasets, the fine-tuned ICAE model (\texttt{ICAE-PT+FT}) achieves a higher BLEU score than the uncompressed baseline.
This suggests that while the model learns to mimic the expert's next action well on a static dataset, this capability does not translate to effective problem-solving when the agent must navigate the consequences of its own actions in a live environment.

\paragraph{Potential Reasons for Performance Degradation.}
We hypothesize several factors may contribute to the degradation in task resolution performance:
\begin{enumerate}
    \item \textbf{Loss of Agentic Capabilities}: The pretraining stage on a general text corpus may weaken the planning or tool-calling capabilities of the base model that are essential for agentic tasks.
    While fine-tuning on agentic data recovers some task-specific abilities, such as code generation (as evidenced by the strong performance on RepoQA in Section~\ref{sec:eval_repoqa}), it may not be sufficient to restore the full spectrum of agentic competence.

    \item \textbf{Imperfect Reconstruction Fidelity}: As discussed in Section~\ref{sec:icae_pretraining_results}, the autoencoding pretraining does not achieve prefect reconstruction (i.e., BLEU scores do not approach $\ge 0.99$).
    In software engineering tasks, even minor inaccuracies in compressed observations can accumulate over a multi-step trajectory and lead to critical failures.

    \item \textbf{Unsuitability of LoRA-FT for Agentic Tasks}: The use of parameter-efficient fine-tuning via LoRA may be insufficient for adapting a model to complex agentic workflows.
    Our results show that the \texttt{Baseline+LoRA-FT} model also performs poorly compared to both the fully fine-tuned and simple baselines, suggesting that LoRA may not be an effective strategy for this specific task, regardless of compression.
    Notably, we have found no research done using LoRA for agentic trajectories, only full-parameter fine-tuning (which performs the best in our experiments).

    \item \textbf{The Challenge of Multi-Turn Interaction}: In single-shot NLP tasks like SQuAD or RepoQA, the context is static and complete.
    In contrast, an agentic task is dynamic: the action at step \(k\) influences the observation at step \(k+1\).
    It is impossible to determine at the moment of compression which pieces of information from an observation will become critical at a future step \(k+i\).
    The compression process, optimized for immediate reconstruction, may discard seemingly unimportant details that are essential for long-term planning and task success.
\end{enumerate}



