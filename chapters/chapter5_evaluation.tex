\chapter{Evaluation}
\label{cha:evaluation}

\section{Datasets}
\label{sec:datasets}

\subsection{SWE-bench}
We have chosen to use the SWE-bench~\cite{jimenez2023swe} dataset for our experiments, as it is a well-known and popular benchmark for evaluating the performance of software engineering agents on real-world tasks.
It contains approximately 3000 tasks sourced from real GitHub issues and pull requests from 12 popular Python repositories (e.g., django, matplotlib).
Each task instance is defined by a "fail-to-pass" scenario: a test suite that fails on the repository state before a fix is applied and passes after.
The agent's goal is to generate a patch that resolves the issue, making the test suite pass.
This setup provides a rigorous and realistic measure of an agent's problem-solving capabilities.
To ensure reproducibility and isolate the agent's performance, the evaluation for each task is conducted within a dedicated Docker container.

We specifically use \textit{SWE-bench Verified}~\cite{chowdhury2024swebenchverified}, a subset of the original benchmark that has been human-validated to address several issues with the original dataset.
The verification process filters out tasks with underspecified problem descriptions, overly specific tests that might reject valid solutions, or problematic development environments.
This results in a more reliable evaluation of an agent's true software engineering capabilities, with the verified subset containing 500 tasks.
Our metric of interest is the percentage of solved instances, which is reported on the verified subset.

\subsection{Agentic Trajectories for Fine-Tuning}
\label{sec:agentic_trajectories_data}
High-quality training data is crucial for fine-tuning capable software engineering agents. 
We require expert demonstrations in the form of agentic trajectories.
To generate them at scale, we use the data collected in~\cite{yang2025swe}.

While SWE-smith provides the framework for generating tasks, we use it to generate expert trajectories for fine-tuning our agent. 
Specifically, we use a powerful teacher model, Claude 3.7 Sonnet, to solve tasks collected by the authors and within the SWE-smith environment.
The teacher model's interactions are recorded as trajectories. We only retain the successful trajectories, resulting in a high-quality dataset of approximately 5000 expert demonstrations.
This filtering step is crucial to ensure that the agent learns from effective problem-solving strategies. 
It is worth noting that there is currently no consensus in the research community on whether including unsuccessful trajectories is beneficial for training~\cite{zeng2024agenttuning, song2024agentbank}.


\paragraph{Interaction Protocol and Tools}
The agent interacts with the environment following a protocol and toolset defined by the SWE-smith setup.
This setup is designed to be minimal yet expressive enough to solve complex software engineering tasks.
The agent is provided with a system prompt that describes the available tools and how to use them. The exact prompt is detailed in Appendix~\ref{app:swe-smith-prompt}.
The agent generates tool calls as plain text, rather than using a model's specific function-calling format.

The available tools are:
\begin{itemize}
    \item \texttt{bash}: A standard shell interface for running commands, allowing the agent to navigate the file system, inspect files, and run tests.
    \item \texttt{submit}: A tool to submit the final patch for evaluation.
    \item \texttt{str\_replace\_editor}: A stateful file editor designed for precise, line-exact operations.
\end{itemize}

The \texttt{str\_replace\_editor} is a critical tool that supports viewing, creating, and editing files. Its state persists across steps, enabling consistent multi-edit workflows. The editor exposes several commands, including \texttt{view}, \texttt{create}, \texttt{str\_replace}, \texttt{insert}, and \texttt{undo\_edit}. To ensure deterministic edits, the \texttt{str\_replace} command requires the \texttt{old\_str} argument to match one or more consecutive lines exactly, including all whitespace. The matched block is then replaced with the content of \texttt{new\_str}. Similarly, the \texttt{insert} command appends content after a specified line number. These precise controls are essential for making targeted changes to code.
We intentionally did not want to complicate our research with more complex scaffolds like SWE-agent~\cite{yang2024swe} to focus on the core of the compression problem.

\subsection{SQuAD}
To evaluate the general compression capabilities of our model beyond agentic tasks, we also test it on the Stanford Question Answering Dataset (SQuAD)~\cite{rajpurkar2016squad}.
SQuAD is a reading comprehension benchmark consisting of over 100,000 question-answer pairs sourced from 536 Wikipedia articles.
The dataset was constructed by asking crowdworkers to pose up to five questions on paragraphs from these articles, where the answer to each question is a direct span of text from the passage.
For example, given the passage ``In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity,'' a corresponding question is ``What causes precipitation to fall?'' with the answer being the single word ``gravity'' extracted from the text.

The choice of SQuAD is motivated by its widespread adoption and focus on extractive answers.
In contrast, the original In-Context Autoencoder work~\cite{ge2023context} used the PWC dataset, which is less common and focuses more on generating longer, abstractive answers rather than extracting specific spans of text.

\subsection{RepoQA}
To assess how well our compression method preserves high-fidelity information in code, we utilize the RepoQA benchmark~\cite{liu2024repoqa}.
RepoQA is specifically designed to evaluate a model's ability to understand and reason about code at the repository level.
Unlike traditional code-related tasks that focus on standalone snippets, RepoQA requires a holistic understanding of entire codebases.
The benchmark's core task, "Searching Needle Function," challenges models to locate a specific function ("needle") within a long, surrounding code context ("haystack") based solely on a natural-language description of its behavior.
This setup moves beyond simple keyword matching and tests for genuine comprehension of both the code's semantics and the description's intent.

The benchmark's framework is flexible, allowing for the creation of evaluation contexts of arbitrary length.
While we conduct experiments with contexts up to 16,000 tokens and observe similar trends, our primary evaluation focuses on a context size of 1024 tokens.
This choice reflects our goal: we are less concerned with testing the limits of raw context length and more interested in verifying that our compressed representation retains the nuanced, structural details of the source code required to succeed at this task.
In our setting, RepoQA serves as a measure of the model's ability not only to decompress but also to generate code.

\section{Quality Metrics}
\label{sec:quality_metrics}
We employ a variety of metrics to evaluate our approach, tailored to the specific demands of each task.
While each dataset has a primary metric suited to its objective, we also report the BLEU score~\cite{papineni2002bleu} score across all evaluations to provide a consistent basis for comparison (see Figure~\ref{fig:bleu_all_datasets}).

\subsection{Task-Level Metrics}
For our main agentic task on SWE-bench Verified, the primary metric is the number of successfully resolved issues (out of 500).
This provides a direct, end-to-end measure of the agent's practical software engineering capabilities.
We also report the mean tool-call generation time to quantify the efficiency gains from our compression method.
We do not use trajectory length as a metric, as it can be misleading; agents can enter repetitive loops that inflate length without making progress.

\subsection{Token-Level Metrics}
For reconstruction and question-answering tasks, we rely on several standard token-level metrics.
Token-wise accuracy measures the fraction of generated tokens that match the ground-truth reference (can be in a teacher-forcing regime or autoregressive mode).
Exact Match (EM) is a stricter metric that scores a prediction as correct only if it is an exact character-for-character match with the reference answer.
The token-level F1 score computes the harmonic mean of precision and recall between the bags of tokens in the prediction and the reference, providing a measure of lexical overlap.
Finally, we use the Bilingual Evaluation Understudy (BLEU) score~\cite{papineni2002bleu} to measure the similarity between generated and reference text.
Specifically, we report BLEU-1, which considers only unigram overlap.
This choice is motivated by the nature of our tasks, particularly with code, where the correctness of individual tokens is more critical than the fluency of longer phrases.
