% ========================================
% CHAPTER 1: INTRODUCTION (MOTIVATION)
% ========================================
\chapter{Introduction}

\textbf{note: here I write motivation. From problem to soluiton.}

\section{The Context Length Challenge in Large Language Models}

The ability of Large Language Models (LLMs) to effectively process long sequences of input text is fundamentally constrained by their architecture.
Specifically, Transformer-based LLMs face inherent limitations due to the self-attention mechanism, which scales quadratically with the number of tokens \cite{vaswani_attention_2017}.
This quadratic complexity restricts the practical context length, posing a significant challenge for tasks requiring extensive history or large documents.

The long context limitation is particularly severe in complex automated scenarios involving agents with many interaction turns.
This restriction is worsened in software engineering (SWE) agent applications, where operational trajectories frequently involve tool calls that generate unnecessarily long outputs (environment observations).
SWE agents must perform tasks such as examining files and directories, reading and modifying parts of files, and navigating complex codebases.
However, pretrained models literally cannot work with sequences longer than N (e.g., 32,000) tokens, which prevents them from efficiently processing the accumulated history generated by these tools.
This is a major problem, as the history of interactions is crucial for the agent to perform the task correctly.

\subsection{Approaches to Extending Context}

To address this challenge, several strategies have been developed.
We can categorize them into four main groups: architectural innovations, extended context windows, explicit compression, and implicit compression.

\paragraph{Architectural Innovations.}
One line of research focuses on modifying the self-attention mechanism to reduce its computational complexity.
Sparse and local/windowed attention patterns reduce pairwise interactions to achieve sub-quadratic cost.
For instance, Longformer combines sliding windows with global tokens to handle long documents \cite{beltagy_longformer_2020}, while BigBird employs block- and mixed-sparsity patterns for similar gains \cite{zaheer_bigbird_2020}.
Linear-time approximations, such as kernelized attention or Linformer's projection method \cite{wang_linformer_2020}, offer further asymptotic improvements.

While these methods offer benefits such as reduced memory footprint, lower computational cost, and effective modeling of local dependencies, they come with notable trade-offs.
Sparse attention patterns can fail to properly route information across distant parts of the sequence when global tokens or connectivity patterns are insufficient.
Additionally, irregular sparsity patterns often lead to hardware inefficiencies, as modern accelerators are optimized for dense operations.
Most critically, these architectural innovations struggle to overcome a notable decline in performance on long contexts, as they may fail to capture critical long-range dependencies that full attention would preserve.

A more recent line of research explores architectures that, while related to Transformers, offer fundamentally different scaling properties.
Recent work has highlighted the duality between Transformers and State Space Models (SSMs) \cite{gu2021efficiently}, a class of models inspired by control theory that can be viewed as a form of recurrent neural network (RNN) \cite{dao2024transformers}.
Architectures like Mamba-2, which builds on this duality, have demonstrated performance competitive with state-of-the-art Transformers on language modeling tasks, especially for very long sequences (e.g., beyond 32,000 tokens), while being significantly faster during inference.
This direction suggests that the future of long-context modeling may lie in hybrid architectures or even a return to modernized recurrent models that avoid the quadratic bottleneck of self-attention.

\paragraph{Extended Context Windows.}
A more direct approach is to leverage newer models that are architecturally designed for very long contexts, often extending to one or two million tokens.
Many of these models utilize advancements like Rotary Position Embeddings (RoPE) \cite{su_roformer_2021} to better handle long-range dependencies.
While this seems like a straightforward solution, it comes with its own set of drawbacks.
Processing extremely long contexts, even with linear-scaling attention mechanisms, is computationally expensive and memory-intensive, leading to high inference latency and cost.
Furthermore, models with large context windows can suffer from the "lost in the middle" problem, where they struggle to effectively utilize information from the middle of a long input sequence \cite{liu2023lost}.

\paragraph{Explicit Compression.}
Another approach is to explicitly compress the context before it is fed to the LLM.
This can be done through methods like retrieval-augmented generation (RAG), which selects relevant passages from a larger corpus \cite{lewis_rag_2020}.
A prominent example is the Retrieval-Enhanced Transformer (RETRO), which conditions on retrieved documents to significantly improve language modeling performance \cite{borgeaud2022retro}.
The main advantage is a controllable computational cost and the ability to incorporate external knowledge.
However, these methods can suffer from selection bias, retrieval errors, and the potential loss of crucial details during the summarization or retrieval process, which can be harmful for downstream tasks that require high fidelity (e.g., code-related tasks).

\paragraph{Implicit Context Condensation.}
This thesis focuses on a fourth approach: implicit context condensation.
This paradigm moves beyond explicit, token-based techniques by utilizing the inherent density of continuous latent spaces.
Instead of relying on discrete representations (tokens), implicit compression focuses on mapping information into a compact set of continuous representations (embeddings).
The core idea is that a text can be represented in different lengths and densities within an LLM while conveying the same essential information.
The goal is to produce task-adapted representations that a model uses during inference, rather than the raw input itself.
This approach promises several advantages: it maintains a tight interface with the model, can reduce latency and memory, and avoids external retrieval steps.
Figure \ref{fig:example1} illustrates the core concept.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/example1.jpeg}
  \caption{Comparison between a base agent and our agent approach for handling large observations exceeding the LLM's context length. The base agent fails when processing long observations directly, while our agent successfully compresses the observation into a fixed set of embeddings before LLM processing, enabling continued task execution.}
  \label{fig:example1}
\end{figure}

The primary goal of context condensation is to leverage this potential density to enable LLM agents to execute tasks involving long chains of reasoning (Chain-of-Thought, CoT) and more steps by condensing the environment observations.
Achieving this condensation improves the model's capability to handle long contexts while offering tangible advantages in improved latency and reduced GPU memory cost during inference.

\subsection{Research Question}

The exploration of implicit context condensation as a solution to the long-context problem leads to the central research question of this thesis:
\vspace{1em}
\begin{center}
\textit{Does implicit context condensation allow for the completion of longer agentic trajectories?}
\end{center}
\vspace{1em}
