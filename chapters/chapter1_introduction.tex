% ========================================
% CHAPTER 1: INTRODUCTION (MOTIVATION)
% ========================================
\chapter{Introduction}


% ========================================
% SECTION 1.1: THE CONTEXT LENGTH CHALLENGE IN LARGE LANGUAGE MODELS (LLMS)
% ========================================
\section{The Context Length Challenge in Large Language Models (LLMs)}

The ability of Large Language Models (LLMs) to effectively process long sequences of input text is fundamentally constrained by their architecture. Specifically, Transformer-based LLMs face inherent limitations due to the self-attention mechanism. Much previous research has attempted to tackle this long context issue through architectural innovations, but these efforts often struggle to overcome a notable decline in performance on long contexts despite reducing computation and memory complexity.

The long context limitation presents a significant practical challenge, particularly in complex automated scenarios. This restriction is exacerbated in software engineering (SWE) agent applications, where operational trajectories frequently involve tool calls that generate unnecessarily long outputs. For instance, custom editing tools available to the agent, such as str\_replace\_editor, are explicitly designed to truncate long command outputs, which are then marked to indicate the missing context. The LLMs' limited context length prevents them from efficiently processing the accumulated history generated by these tools.

Context compression offers a novel approach to addressing this issue, motivated by the observation that a text can be represented in different lengths in an LLM while conveying the same information. For example, the same information might be represented by a context length of 2,572 characters, 512 (sub-)words, or a compact 128 memory slots, without necessarily affecting the accuracy of the model's subsequent response.

The core goal of context condensation is precisely to leverage this potential density to enable LLM agents to execute tasks involving long chains of reasoning (or Chain-of-Thought, CoT) and more steps by condensing the environment observations. Achieving this condensation improves the model's capability to handle long contexts while offering tangible advantages in improved latency and reduced GPU memory cost during inference. For instance, empirical testing shows that compression using the In-context Autoencoder (ICAE) framework can achieve over $2\times$ to $3.6\times$ inference speedup in total time, especially in compute-intensive scenarios.


% ========================================
% SECTION 1.2: REFRAMING THE GOAL: FEATURE EXTRACTION VS. COMPRESSION
% ========================================
\section{Reframing the Goal: Feature Extraction vs. Compression}

When developing a context condensation strategy, the definition of success must be carefully framed. The approach investigated utilizes the In-context Autoencoder (ICAE), which leverages the power of an LLM to compress a long context into short compact memory slots that can be directly conditioned upon by the decoder LLM.

However, the approach should be redefined not merely as "compression," which often implies a robust, high-ratio data reduction, but rather as "summarization" or "fixed length feature extraction," due to a core methodological constraint. This constraint arises from the hardcoded, non-robust nature of the intended output length (e.g., aiming for 256 tokens in general discussion).

This reframing is critical because lossless compression typically struggles to achieve ratios exceeding $10\times$. Under high compression ratios (lossy compression), the assumption that "all tokens in the context are equally important"—an assumption intrinsically aligned with lossless autoencoding training—is violated. When the information carrier capacity is limited, the compression mechanism should ideally focus only on the most important tokens, which conflicts with the uniform coverage implied by fixed-length encoding.

The fundamental mechanism supporting this goal is the relative density of different representation spaces. The latent space of embeddings is "much denser than the discrete space of tokens," which is the underlying justification for learning context condensation. Therefore, the thesis investigates how condensing environment observations (which contain irrelevant or redundant information) into continuous representations (embeddings/memory slots) affects agent performance and efficiency when addressing the context length challenge.

Wichtige Informationen finden sich in \cref{tab:wonderful-table}.

\begin{table}[hbt]
  \centering
  \begin{tabular}{rl}
    \toprule%
    \textbf{Name}& \textbf{Place of Birth}\\ \midrule
    Gauß & Braunschweig\\
    Euler & Basel\\
    Edmonds & Washington, D.\@C.\@\\
    \bottomrule
  \end{tabular}

  \caption{A most wonderful table}%
  \label{tab:wonderful-table}
\end{table}
