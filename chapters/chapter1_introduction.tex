% ========================================
% CHAPTER 1: INTRODUCTION (MOTIVATION)
% ========================================
\chapter{Introduction}
\label{cha:introduction}

\section{The Context Length Challenge in Large Language Models}

The ability of Large Language Models (LLMs) to effectively process long sequences of input text is fundamentally constrained by their architecture.
Specifically, Transformer-based LLMs face inherent limitations due to the self-attention mechanism, which scales quadratically with the number of tokens \cite{vaswani2017attention}.
This quadratic complexity restricts the practical context length, posing a significant challenge for tasks requiring extensive history or large documents.

The long context limitation is particularly severe in complex automated scenarios involving agents with many interaction turns.
This restriction is worsened in software engineering (SWE) agent applications, where operational trajectories frequently involve tool calls that generate unnecessarily long outputs (environment observations) \cite{jimenez2023swe, yang2024swe}.
SWE agents must perform tasks such as examining files and directories, reading and modifying parts of files, and navigating complex codebases \cite{yang2025swe}.
However, pretrained models literally cannot work with sequences longer than N (e.g., 32,000) tokens, which prevents them from efficiently processing the accumulated history generated by these tools.
This is a major problem, as the history of interactions is crucial for the agent to perform the task correctly.

\subsection{Approaches to Extending Context}

To address this challenge, several strategies have been developed.
We categorize them into four main groups: architectural innovations, extended context windows, explicit compression, and implicit compression.

\paragraph{Architectural Innovations.}
One line of research focuses on modifying the self-attention mechanism to reduce its computational complexity.
Sparse and local/windowed attention patterns reduce pairwise interactions to achieve sub-quadratic cost.
For instance, Longformer combines sliding windows with global tokens to handle long documents \cite{beltagy2020longformer}, while BigBird employs block- and mixed-sparsity patterns for similar gains \cite{zaheer2020big}.
Linear-time approximations, such as kernelized attention or Linformer's projection method \cite{wang2020linformer}, offer further asymptotic improvements.

While these methods offer benefits such as reduced memory footprint, lower computational cost, and effective modeling of local dependencies, they come with notable trade-offs.
Sparse attention patterns can fail to properly route information across distant parts of the sequence when global tokens or connectivity patterns are insufficient \cite{zhuang2023survey}.
Additionally, irregular sparsity patterns often lead to hardware inefficiencies, as modern accelerators are optimized for dense operations \cite{dao2022flashattention}.
Most critically, these architectural innovations struggle to overcome a notable decline in performance on long contexts, as they may fail to capture critical long-range dependencies that full attention would preserve.

A more recent line of research explores architectures that, while related to Transformers, offer fundamentally different scaling properties.
Recent work has highlighted the connection between Transformers and State Space Models (SSMs) \cite{gu2021efficiently}, a class of models inspired by control theory that can be viewed as a form of recurrent neural network (RNN) \cite{dao2024transformers}.
Architectures like Mamba-2, which builds on this duality, have demonstrated performance competitive with state-of-the-art Transformers on language modeling tasks, especially for very long sequences (e.g., beyond 32,000 tokens), while being significantly faster during inference.
This direction suggests that the future of long-context modeling may lie in hybrid architectures or even a return to modernized recurrent models that avoid the quadratic bottleneck of self-attention.

\paragraph{Extended Context Windows.}
A more direct approach is to leverage newer models that are architecturally designed for very long contexts, often extending to one or two million tokens.
Many of these models utilize advancements like Rotary Position Embeddings (RoPE) \cite{su2024roformer} to better handle long-range dependencies.
While this seems like a straightforward solution, it comes with its own set of drawbacks.
Processing extremely long contexts, even with linear-scaling attention mechanisms, is computationally expensive and memory-intensive, leading to high inference latency and cost.
Furthermore, models with large context windows can suffer from the "lost in the middle" problem, where they struggle to effectively utilize information from the middle of a long input sequence \cite{liu2024lost}.

\paragraph{Explicit Compression.}
Another approach is to explicitly compress the context before it is fed to the LLM.
This can be done through methods like retrieval-augmented generation (RAG), which selects relevant passages from a larger corpus \cite{lewis2020retrieval}.
A prominent example is the Retrieval-Enhanced Transformer (RETRO), which conditions on retrieved documents to significantly improve language modeling performance \cite{borgeaud2022improving}.
The main advantage is a controllable computational cost and the ability to incorporate external knowledge.
However, these methods can suffer from selection bias, retrieval errors, and the potential loss of crucial details during the summarization or retrieval process, which can be harmful for downstream tasks that require high fidelity (e.g., code-related tasks).

\paragraph{Implicit Context Condensation.}
This thesis focuses on a fourth approach: implicit context condensation.
This paradigm moves beyond explicit, token-based techniques by utilizing the inherent density of continuous latent spaces.
Instead of relying on discrete representations (tokens), implicit compression focuses on mapping information into a compact set of continuous representations (embeddings).
The core idea is that a text can be represented in different lengths and densities within an LLM while conveying the same essential information \cite{chevalier2023adapting}.
The goal is to produce task-adapted representations that a model uses during inference, rather than the raw input itself.
This approach promises several advantages: it maintains a tight interface with the model, can reduce latency and memory, and avoids external retrieval steps.
Figure \ref{fig:example1} illustrates the core concept.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/example1.jpeg}
  \caption{Comparison between a base agent and our agent approach for handling large observations exceeding the LLM's context length. The base agent fails when processing long observations directly, while our agent successfully compresses the observation into a fixed set of embeddings before LLM processing, enabling continued task execution.}
  \label{fig:example1}
\end{figure}

The primary goal of context condensation is to leverage this potential density to enable LLM agents to execute tasks involving long chains of reasoning (Chain-of-Thought, CoT) and more steps by condensing the environment observations.
Achieving this condensation improves the model's capability to handle long contexts while offering tangible advantages in improved latency and reduced GPU memory cost during inference.

\subsection{Research Questions}
The exploration of implicit context condensation as a solution to the long-context problem leads to the central research questions of this thesis:
\vspace{1em}
\begin{center}
\label{rq:1}
\textbf{RQ1:} \textit{How does implicit context condensation influence the efficiency of LLM-based agents when applied to software engineering tasks?}
\end{center}
\vspace{1em}
A secondary question explores whether the results of this approach on general texts transfer to the domains of coding and agentic software engineering:
\vspace{1em}
\begin{center}
\label{rq:2}
\textbf{RQ2:} \textit{How does the performance of implicit context condensation on standard NLP benchmarks transfer to software engineering tasks, both single-shot and agentic?}
\end{center}
\vspace{1em}

\subsection{Thesis Outline}

The \textbf{\hyperref[cha:introduction]{Introduction}} begins by establishing the context length challenge in LLMs and introducing implicit context condensation as a potential solution, leading to the formulation of the core research questions.
Following this, the \textbf{\hyperref[cha:background]{Background}} provides the necessary foundation, with a focus on Transformer encoding and positional bias, the mechanics of Low Rank Adaptation (LoRA), and the operational principles of agentic workflows.
The subsequent \textbf{\hyperref[cha:related_work]{Related Work}} situates this thesis within the existing literature, offering an analysis of prior approaches to context management, including both explicit and implicit compression variants.
The core method is detailed in the \textbf{\hyperref[cha:methodology]{Methodology}}, which explains the adaptation of the In-Context Autoencoder (ICAE) for the agentic setting.
Next, the \textbf{\hyperref[cha:evaluation]{Evaluation}} chapter outlines the datasets used (SWE-bench Verified, SQuAD, and RepoQA) and the metrics for assessment, including task-level success rates and token-level metrics (BLEU, F1 score).
The results of this study are presented in the \textbf{\hyperref[cha:experiments]{Experiments}} chapter, which has the outcomes of experiments on text and code reconstruction, QA task, and SWE-bench Verified.
A critical analysis of the factors contributing to these outcomes is provided in the \textbf{\hyperref[cha:discussion]{Discussion}}.
The thesis then addresses its \textbf{\hyperref[cha:limitations]{Limitations and Future Work}}, which includes the constraints of fixed-length condensation, the KV-caching and the potential bottleneck of LoRA-based fine-tuning.
Finally, the \textbf{\hyperref[cha:conclusion]{Conclusion}} summarizes the key contributions of the thesis.
The results confirm that the method enables longer agentic trajectories and reduces inference time (\hyperref[rq:1]{RQ1}). Regarding the transferability of performance (\hyperref[rq:2]{RQ2}), the findings show effective transfer to code-related tasks, whereas a reduction in quality is observed in agentic software engineering tasks.
