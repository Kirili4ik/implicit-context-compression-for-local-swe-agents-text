% ========================================
% CHAPTER 1: INTRODUCTION (MOTIVATION)
% ========================================
\chapter{Introduction}


% ========================================
% SECTION 1.1: THE CONTEXT LENGTH CHALLENGE IN LARGE LANGUAGE MODELS (LLMS)
% ========================================
\section{The Context Length Challenge in Large Language Models (LLMs)}

The ability of Large Language Models (LLMs) to effectively process long sequences of input text is fundamentally constrained by their architecture.
Specifically, Transformer-based LLMs face inherent limitations due to the self-attention mechanism, which scales quadratically with the number of tokens.
Much previous research has attempted to tackle this long context issue through architectural innovations, but these efforts often struggle to overcome a notable decline in performance on long contexts despite reducing computation and memory complexity.
While alternative attention mechanisms can make scaling closer to linear, they typically cannot achieve true linear scaling without quality drops on longer sequences.

The long context limitation presents a significant practical challenge, particularly in complex automated scenarios involving agents with many interaction turns.
This restriction is worsened in software engineering (SWE) agent applications, where operational trajectories frequently involve tool calls that generate unnecessarily long outputs.
SWE agents must perform tasks such as examining files and directories, reading and modifying parts of files, and navigating complex codebases.
However, pretrained models literally cannot work with sequences longer than N (e.g., 32,000) tokens, which prevents them from efficiently processing the accumulated history generated by these tools.

Context compression offers a novel approach to addressing this issue, motivated by the observation that a text can be represented in different lengths in an LLM while conveying the same information.
For example, the same information might be represented in various formats and densities without necessarily affecting the accuracy of the model's subsequent response.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/example1.jpeg}
  \caption{Comparison between base agent and our agent approaches for handling large observations exceeding LLM context length. The base agent fails when processing observations that are too long directly, while our agent successfully compresses the observation to 256 embeddings before LLM processing, enabling continued task execution.}
  \label{fig:example1}
\end{figure}

The core goal of context condensation is precisely to leverage this potential density to enable LLM agents to execute tasks involving long chains of reasoning (or Chain-of-Thought, CoT) and more steps by condensing the environment observations.
Achieving this condensation improves the model's capability to handle long contexts while offering tangible advantages in improved latency and reduced GPU memory cost during inference.



% ========================================
% SECTION 1.2: Implicit Context Condensation
% ========================================

\section{Implicit and Explicit Compression}

The core concept of compression, motivated by the observation that information can be represented in different forms and densities, leads directly to the discussion and comparison between implicit and explicit approaches.
Implicit compression moves beyond explicit, token-based techniques by utilizing the inherent density of continuous latent spaces.

Instead of relying on discrete representations, implicit compression focuses on mapping information into continuous representations (embeddings).
This is possible because continuous latent spaces are inherently much denser than discrete spaces.
Our goal is to enable more efficient processing of information by successfully leveraging these dense representations.