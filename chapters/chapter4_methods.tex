% ========================================
% CHAPTER 4: METHODS (CONCEPTUAL: ARCHITECTURE AND TRAINING)
% ========================================
\chapter{Methods}
\label{cha:methods}

% NOTE: This chapter describes the conceptual framework of applying ICAE to agentic trajectories.
% It avoids experimental specifics, results, and detailed configurations, which are covered in subsequent chapters.

% ========================================
% SECTION 4.1: ICAE FOR AGENTIC CONTEXT MANAGEMENT
% ========================================
\section{ICAE for Agentic Context Management}

The In-Context Autoencoder (ICAE) \cite{ge2023context} consists of two modules: a trainable encoder (typically a LoRA-adapted LLM) and a fixed decoder (the base LLM itself).
The encoder processes a long context and generates a fixed number of learnable memory tokens.
This design turns a long, potentially unwieldy context into a compact representation that the decoder can efficiently consume, improving latency and memory footprint while preserving fidelity for downstream tasks.
The number of memory tokens controls the compression ratio, and their placement influences how the decoder accesses the stored information \cite{ge2023context}.

Figure~\ref{fig:icae} depicts the encoderâ€“decoder split.
The encoder ingests the full context and produces memory tokens.
The frozen decoder then receives these tokens and a prompt to generate a continuation.
During pretraining, the encoder is optimized to enable the decoder to reconstruct the original text.
During fine-tuning, the objective shifts to solving a downstream task (e.g., generating a tool call) using the compressed representation.
Parameter-efficient methods like LoRA \cite{hu2022lora} are used to adapt the encoder while keeping the base model's capabilities intact.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/icae.jpeg}
  \caption{In-Context Autoencoder (ICAE) framework architecture.}
  \label{fig:icae}
\end{figure}

Our main contribution is the adaptation and application of this framework to an agentic setting.
In this scenario, an agent interacts with an environment over multiple turns, generating a trajectory of actions and observations.
Our method uses ICAE to compress long observations, keeping the overall context manageable without losing critical information.

% ========================================
% SECTION 4.2: TRAINING METHODOLOGY FOR AGENTIC ICAE
% ========================================
\section{Training Methodology for Agentic ICAE}

The process for training ICAE in an agentic scenario is depicted in Figure~\ref{fig:icae-agent-training-overview}.
The training data consists of pre-recorded expert trajectories, which are sequences of alternating actions and observations.

At the start of the interaction, an uncompressed user prompt is provided.
Then, a trajectory unfolds as an agent takes an action (tool call), which is sent to an environment, and receives an observation in return.
This observation becomes input for the next step.
During the trajectory, compression is applied to every long observation, ensuring that the decoder model never processes lengthy raw text.
Instead, it operates on compact embedding representations.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/mega-1.jpeg}
  \caption{Overview of applying ICAE to agentic trajectories during fine-tuning.}
  \label{fig:icae-agent-training-overview}
\end{figure}

Figure~\ref{fig:icae-agent-training-step} illustrates a single fine-tuning step.
The observation from the environment is passed to the ICAE encoder, which produces a compressed representation.
This representation, along with the prior conversation history, is fed to the frozen decoder to generate the next tool call.
The training loss is the cross-entropy between the generated tool call and the reference action from the expert trajectory.
This loss is backpropagated through both the decoder and encoder to update only the encoder's LoRA weights, while the base model weights remain frozen.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/mega-2.jpeg}
  \caption{A single fine-tuning step for the agentic ICAE model.}
  \label{fig:icae-agent-training-step}
\end{figure}

\subsection{Pretraining (PT)}
The first stage follows the original ICAE formulation~\cite{ge2023context}, pretraining the encoder on a large, general-purpose text corpus.
Two self-supervised objectives are used in a 50/50 mix:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Autoencoding (AE)}, where ICAE restores the original input text from its memory slots. This task is signaled by a special \texttt{<AE>} token, as conceptually illustrated in Figure~\ref{fig:icae}.
    \item \textbf{Language Modeling (LM)}, which predicts the continuation of a context to improve generalization, without the use of any special tokens.
\end{enumerate}
During this stage, only the encoder's LoRA weights are trained.
The goal is to teach the encoder to produce embeddings from which the frozen decoder can effectively reconstruct or continue text.

\subsection{Fine-Tuning (FT)}
After pretraining, the ICAE encoder is fine-tuned on a dataset of agentic trajectories.
Again, only the encoder's LoRA weights are updated.
The objective is to maximize the probability of generating the correct agent action (i.e., tool call) conditioned on the memory slots (for compressed observations) and the rest of the history.

During training, we optimize over single-step transitions.
At a timestep \(k\), the encoder compresses the observation \(o_{k-1}\).
The decoder then generates action \(a_k\) from the compressed history.
The loss from \(a_k\) is backpropagated to update the encoder's LoRA weights.
Crucially, whenever an observation exceeds a predefined threshold (e.g., 256 tokens), the encoder compresses it into a fixed set of memory tokens.
This ensures the model never processes the full raw text of long observations, allowing it to handle arbitrarily long trajectories without exceeding context limits.

For example, consider the following trajectory:
\begin{enumerate}
  \item \textbf{System prompt} (text): initial instructions and tool descriptions.
  \item \textbf{Task description} (text): user-provided issue or goal.
  \item \textbf{Action 1} (text): e.g., \texttt{bash: ls -la}.
  \item \textbf{Observation 1} (short text, $<256$ tokens): directory listing, kept as-is.
  \item \textbf{Action 2} (text): e.g., \texttt{str\_replace\_editor: view file.py}.
  \colorbox{yellow}{\textbf{Observation 2} (long text): entire file content, compressed into memory tokens.}
  \item \textbf{Action 3} (text): e.g., \texttt{str\_replace\_editor: str\_replace ...}.
  \colorbox{yellow}{\textbf{Observation 3} (long text): edit confirmation with context, again compressed into memory tokens.}
  \item \textbf{Action 4} (text): e.g., \texttt{bash: pytest}.
  \item \textbf{Observation 4} (short text): test results summary, kept as text.
  \item \textbf{Action 5} (text): \texttt{submit}.
  \item \textbf{End}.
\end{enumerate}
In this example, the encoder is applied twice (to compress observations 2 and 3, highlighted in yellow), while the decoder generates five actions.
The model then is able to predict actions from a history where long observations have been replaced by their compact memory representations.

\subsection{Training Process and Model Variants}

Figure~\ref{fig:training-process-overview} provides a comprehensive overview of the full training and evaluation pipeline, illustrating how each of our model variants is derived. The starting point for all variants is a standard, pretrained \texttt{Qwen3-8B} model~\cite{yang2025qwen3}, which we refer to as the \texttt{Baseline}. This is a ready-to-use model, not one with random weights.

The figure illustrates two parallel training paths. The first path is for our ICAE model. The \texttt{Baseline} model is used to initialize the ICAE encoder and decoder. In the Pretraining (PT) stage, we train the LoRA weights of the encoder on the SlimPama-6B dataset~\cite{weber2024redpajama}, resulting in the \texttt{ICAE-PT} model. This model is then fine-tuned on the agentic trajectories dataset, yielding the final \texttt{ICAE-PT+FT} model. Crucially, for both ICAE training stages, only the encoder's LoRA weights are updated, while the base Qwen3 model used as the decoder remains frozen.

The second path is for a comparative baseline. The original \texttt{Baseline} Qwen3 model is directly fine-tuned with LoRA on the agentic trajectories dataset. This produces the \texttt{Baseline+FT} model. This allows us to compare our two-stage ICAE approach against a standard parameter-efficient fine-tuning of a base language model on the target task.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{graphs/overall-names.jpg}
  \caption{Full training process overview, illustrating the derivation of \texttt{Baseline}, \texttt{ICAE-PT}, \texttt{Baseline+FT}, and \texttt{ICAE-PT+FT} models.}
  \label{fig:training-process-overview}
\end{figure}