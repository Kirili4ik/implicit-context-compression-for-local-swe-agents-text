% ========================================
% CHAPTER 4: METHODS (CONCEPTUAL: ARCHITECTURE AND DATA)
% ========================================
\chapter{Methods}


% ========================================
% SECTION 4.1: ICAE MODEL ARCHITECTURE AND COMPONENTS
% ========================================
\section{ICAE Model Architecture and Components}

The ICAE \cite{ge_-context_2024} structure consists of two modules: a lightweight encoder (LoRA-adapted LLM) and a fixed decoder (the target LLM, e.g., Qwen or Llama).
The encoder processes the long context and appends learnable memory tokens (e.g., 128 tokens for 4x compression) to obtain memory slots.
The decoder uses these memory slots, conditioned by prompts, to generate responses on behalf of the original context.


% ========================================
% SECTION 4.2: SELF-SUPERVISED PRETRAINING OBJECTIVES
% ========================================
\section{Self-Supervised Pretraining Objectives}

Autoencoding (AE): The ICAE \cite{ge_-context_2024} is trained to restore the original input text from its memory slots, prompted by a special token [AE].
Language Modeling (LM) / Text Continuation: An additional objective where the model predicts the continuation of the context, prompted by a special token [LM].
This improves generalization and prevents overfitting to the AE task.


% ========================================
% SECTION 4.3: INSTRUCTION FINE-TUNING FOR DOWNSTREAM TASKS
% ========================================
\section{Instruction Fine-Tuning for Downstream Tasks}

After pretraining, the ICAE \cite{ge_-context_2024} is fine-tuned using instruction data to enhance the interaction between the memory slots and various prompts, enabling the target LLM to produce desirable responses.
The fine-tuning loss aims to maximize the probability of generating the correct response conditioned on the memory slots and the prompt.


% ========================================
% SECTION 4.4: EXPERIMENTAL DATASETS AND CONFIGURATION
% ========================================
\section{Experimental Datasets and Configuration}

General Text Data: The Pile dataset is used for pretraining.
QA and Instruction Data: The PWC (Prompt-with-Context) dataset, consisting of thousands of (context, prompt, response) samples, is used for instruction fine-tuning and evaluation.
SQuAD is used for offline quality measurement and fast debugging.
Agentic Data: The SWE-bench Verified dataset is used to assess end-to-end performance on complex software engineering tasks, utilizing trajectories generated by a strong teacher model (e.g., GPT-5).
