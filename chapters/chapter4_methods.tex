% ========================================
% CHAPTER 4: METHODS (CONCEPTUAL: ARCHITECTURE AND TRAINING)
% ========================================
\chapter{Methodology}
\label{cha:methodology}

This chapter presents the comprehensive methodology for developing and evaluating an In-Context Autoencoder (ICAE) designed for agentic context management.
The chapter begins by providing a high-level overview of the entire training pipeline, illustrating how our proposed model and its variants are derived for comparative analysis.
Following this, we dive into the core architectural principles of applying the ICAE framework to compress the conversational history of agentic trajectories.
The subsequent sections detail the two-stage training process, which starts with a self-supervised pretraining phase on a large-scale text corpus to teach the model effective compression, and transitions to a fine-tuning phase on a specialized dataset of agentic trajectories to adapt the model for the downstream task.

\label{ex:trajectory-compression}
As an example, we would use the following trajectory (you can also see Figure~\ref{fig:icae-agent-training-overview} for a basic visual representation of the trajectory):
\begin{enumerate}
  \item \textbf{System prompt} (text): initial instructions and tool descriptions.
  \item \textbf{Task description} (text): user-provided issue or goal.
  \item \textbf{Action 1} (text): e.g., \texttt{bash: ls -la}.
  \item \textbf{Observation 1} (short text, $<256$ tokens): directory listing, kept as-is.
  \item \textbf{Action 2} (text): e.g., \texttt{str\_replace\_editor: view file.py}.
  \item \colorbox{yellow}{\textbf{Observation 2} (long text): entire file content, compressed into memory tokens.}
  \item \textbf{Action 3} (text): e.g., \texttt{str\_replace\_editor: str\_replace ...}.
  \item \colorbox{yellow}{\textbf{Observation 3} (long text): edit confirmation with context, again compressed into memory tokens.}
  \item \textbf{Action 4} (text): e.g., \texttt{bash: pytest}.
  \item \textbf{Observation 4} (short text): test results summary, kept as text.
  \item \textbf{Action 5} (text): \texttt{submit}.
  \item \textbf{End}.
\end{enumerate}

\section{Overview of the Training Process and Model Variants}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/overall-names.jpg}
  \caption{Full training process overview, illustrating the derivation of model variants. The diagram shows two parallel training paths starting from the pretrained \texttt{Qwen3-8B} baseline. The first path (top) shows the ICAE approach: the baseline initializes both encoder and decoder, then pretraining (PT) on SlimPama-6B produces \texttt{ICAE-PT}, followed by fine-tuning (FT) on agentic trajectories to yield \texttt{ICAE-PT+FT}. Only encoder LoRA weights are trained while the decoder remains frozen. The second path (bottom) shows direct LoRA fine-tuning of the baseline on agentic trajectories, producing \texttt{Baseline+FT}. Thus, we receieve all 4 main model variants for comparison.}
  \label{fig:training-process-overview}
\end{figure}

Figure~\ref{fig:training-process-overview} provides a comprehensive overview of the full training and evaluation pipeline, illustrating how each of our model variants is derived.

The starting point for all variants is a standard, pretrained \texttt{Qwen3-8B} model~\cite{yang2025qwen3}, which we refer to as the \texttt{Baseline}. This is a ready-to-use model, not one with random weights.
So, basically, the model in the \texttt{Baseline} is already a pretrained model, but we stick with the naming of the process in \textcite{ge2023context} for consistency and name the 2 stages as Pretraining (PT) and Fine-Tuning (FT).

There are two parallel training paths. The first path is for our ICAE model. The \texttt{Baseline} model is used to initialize the ICAE encoder and decoder. In the Pretraining (PT) stage, we train the LoRA weights of the encoder on the SlimPama-6B dataset~\cite{weber2024redpajama}, resulting in the \texttt{ICAE-PT} model. This model is then fine-tuned on the agentic trajectories dataset, yielding the final \texttt{ICAE-PT+FT} model. Crucially, for both ICAE training stages, only the encoder's LoRA weights are updated, while the base Qwen3 model used as the decoder remains frozen.

The second path is for a comparative baseline. The original \texttt{Baseline} Qwen3 model is directly fine-tuned with LoRA on the agentic trajectories dataset. This produces the \texttt{Baseline+FT} model. This allows us to compare our two-stage ICAE approach against a standard parameter-efficient fine-tuning of a base language model on the target task.

% ========================================
% SECTION 4.1: ICAE FOR AGENTIC CONTEXT MANAGEMENT
% ========================================
\section{ICAE for Agentic Context Management}

The In-Context Autoencoder (ICAE) \cite{ge2023context} consists of two modules: a trainable encoder (typically a LoRA-adapted LLM) and a fixed decoder (the base LLM itself).
The encoder processes a long context and generates a fixed number of learnable memory tokens.
This design turns a long, potentially unwieldy context into a compact representation that the decoder can efficiently consume.
The number of memory tokens controls the compression ratio, and their placement influences how the decoder accesses the stored information \cite{ge2023context}.

Figure~\ref{fig:icae} depicts the encoder–decoder split.
The encoder ingests the full context and produces memory tokens.
The frozen decoder then receives these tokens and a prompt to generate a continuation.
During pretraining, the encoder is optimized to enable the decoder to reconstruct the original text.
During fine-tuning, the objective shifts to solving a downstream task (e.g., generating a tool call) using the compressed representation.
Parameter-efficient methods like LoRA \cite{hu2022lora} are used to adapt the encoder while keeping the base model's capabilities intact.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/icae.jpeg}
  \caption{In-Context Autoencoder (ICAE) framework architecture.}
  \label{fig:icae}
\end{figure}

Our main contribution is the adaptation and application of this framework to an agentic setting.
In this scenario, an agent interacts with an environment over multiple turns, generating a trajectory of actions and observations.
Our method uses ICAE to compress long observations, keeping the overall context manageable without losing critical information.

% ========================================
% SECTION 4.2: TRAINING METHODOLOGY FOR AGENTIC ICAE
% ========================================
\section{Training Methodology for Agentic ICAE}

The process for training ICAE in an agentic scenario is depicted in Figure~\ref{fig:icae-agent-training-overview}.
The training data consists of pre-recorded agent trajectories (see an example of a trajectory in Example~\ref{ex:trajectory-compression}), which are sequences of alternating actions and observations, generated by an expert agent (in our case, Claude 3.7 Sonnet, see Figure~\ref{fig:icae-agent-training-overview}).
The collection of these trajectories is described in Section~\ref{sec:agentic_trajectories_data}.

At the start of the interaction, an uncompressed user prompt is provided (e.g., system prompt and task description in Example~\ref{ex:trajectory-compression}).
Then, a trajectory unfolds as an agent takes an action (tool call/action, e.g., \texttt{bash: ls -la} in Example~\ref{ex:trajectory-compression}), which is sent to an environment, and receives an observation in return.
This observation becomes input for the next step.
During the trajectory, compression is applied to every long observation (e.g., observations 2 and 3 in Example~\ref{ex:trajectory-compression}), ensuring that the decoder model never processes lengthy raw text.
Instead, it operates on compact embedding representations.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/mega-1.jpeg}
  \caption{Overview of applying ICAE to agentic trajectories during fine-tuning.}
  \label{fig:icae-agent-training-overview}
\end{figure}

Figure~\ref{fig:icae-agent-training-step} illustrates a single fine-tuning step.
The observation from the environment is passed to the ICAE encoder, which produces a compressed representation.
This representation, along with the prior conversation history, is fed to the frozen decoder to generate the next tool call.
The training loss is the cross-entropy between the generated tool call and the reference action from the expert trajectory.
This loss is backpropagated through both the decoder and encoder to update only the encoder's LoRA weights, while the base model weights remain frozen.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/mega-2.jpeg}
  \caption{A single fine-tuning step for the agentic ICAE model.}
  \label{fig:icae-agent-training-step}
\end{figure}

\subsection{Pretraining (PT)}
The first stage follows the original ICAE formulation~\cite{ge2023context}, pretraining the encoder on a large, general-purpose text corpus.
Two self-supervised objectives are constructed from text sequences in equal proportion (with a 50\% probability for each):
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Autoencoding (AE)}, where ICAE restores the original input text from its memory slots. This task is signaled by a special \texttt{<AE>} token, as conceptually illustrated in Figure~\ref{fig:icae}.
    \item \textbf{Language Modeling (LM)}, which predicts the continuation of a context to improve generalization, without the use of any special tokens.
\end{enumerate}
During this stage, only the encoder's LoRA weights are trained.
The goal is to teach the encoder to produce embeddings from which the frozen decoder can effectively reconstruct or continue text.

\subsection{Fine-Tuning (FT)}
After pretraining, the ICAE encoder is fine-tuned on a dataset of agentic trajectories (for more details, see Section~\ref{sec:agentic_trajectories_data}).
It should be noted that only the encoder's LoRA weights are updated.
The objective is to maximize the probability of generating the correct agent action (i.e., tool call) conditioned on the memory slots (for compressed observations) and the rest of the previous trajectory history.
Each turn in a trajectory is treated as a separate training sample, where the model predicts the next action given the trajectory history up to that point. The last observation is compressed and trained, while all the previous observations are saved during the previous turns and fed in as compressed embeddings.

During training, we optimize over single-step transitions.
At a timestep \(k\), the encoder compresses the observation \(o_{k-1}\).
The decoder then generates action \(a_k\) from the compressed history.
The loss from \(a_k\) is backpropagated to update the encoder's LoRA weights.
Importantly, backpropagation is performed only through a single turn — we do not backpropagate through the entire trajectory history as this would not really feasible due to the memory constraints.
Crucially, whenever an observation exceeds a predefined threshold (e.g., 256 tokens), the encoder compresses it into a fixed set of memory tokens.
This ensures the model never processes the full raw text of long observations, allowing it to handle arbitrarily long trajectories without exceeding context limits.

In the \ref{ex:trajectory-compression}q, the encoder is applied twice (to compress observations 2 and 3, highlighted in yellow), while the decoder generates five actions.
The model then is able to predict actions from a history where long observations have been replaced by their compact memory representations.

\subsection{Disabling "Thinking" for Tool Use}
Finally, a specific configuration choice for our agentic experiments involves the model's reasoning mode. The Qwen3 family includes a "thinking" feature designed for complex chain-of-thought reasoning. However, for our tool-use objectives, we explicitly disable this feature to prioritize direct action generation and to simplify the context management pipeline. This decision is analyzed further in the ablation study in Appendix~\ref{sec:disabling_thinking}.
