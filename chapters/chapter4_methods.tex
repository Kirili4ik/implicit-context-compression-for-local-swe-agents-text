% ========================================
% CHAPTER 4: METHODS (CONCEPTUAL: ARCHITECTURE AND TRAINING)
% ========================================
\chapter{Methodology}
\label{cha:methodology}

This chapter presents the comprehensive methodology for developing and evaluating an In-Context Autoencoder (ICAE) designed for agentic context management.
The chapter begins by providing a high-level overview of the entire training pipeline, illustrating how our proposed model and its variants are derived for comparative analysis.
Following this, we dive into the core architectural principles of applying the ICAE framework to compress the conversational history of agentic trajectories.
The subsequent sections detail the two-stage training process, which begins with a self-supervised pretraining phase on a large-scale text corpus to teach the model effective compression, followed by a fine-tuning phase on a specialized dataset of agentic trajectories to adapt the model for the downstream task.

\label{ex:trajectory-compression}
As an example, we would use the following trajectory (you can also see Figure~\ref{fig:icae-agent-training-overview} for a basic visual representation of the trajectory):
\begin{enumerate}
  \item \textbf{System prompt} (text): initial instructions and tool descriptions (Appendix~\ref{app:swe-smith-prompt}).
  \item \textbf{Task description} (text): user-provided issue or goal (Appendix~\ref{app:swe-smith-first-user-message}).
  \item \textbf{Action 1} (text): e.g., \texttt{bash: ls -la}.
  \item \textbf{Observation 1} (short text, $<256$ tokens): directory listing, kept as-is.
  \item \textbf{Action 2} (text): e.g., \texttt{str\_replace\_editor: view file.py}.
  \item \colorbox{yellow}{\textbf{Observation 2} (long text): entire file content, compressed into memory tokens.}
  \item \textbf{Action 3} (text): e.g., \texttt{str\_replace\_editor: str\_replace ...}.
  \item \colorbox{yellow}{\textbf{Observation 3} (long text): edit confirmation with context, again compressed into memory tokens.}
  \item \textbf{Action 4} (text): e.g., \texttt{bash: pytest}.
  \item \textbf{Observation 4} (short text): test results summary, kept as text.
  \item \textbf{Action 5} (text): \texttt{submit}.
  \item \textbf{End}.
\end{enumerate}

In the above example, the encoder is applied twice (to compress observations 2 and 3, highlighted in yellow), while the decoder generates five actions.
The model is then able to predict actions from a history where long observations have been replaced by their compact memory representations.
For more details and rationale on this, see Section~\ref{sec:fine-tuning}.

\section{Overview of the Training Process and Model Variants}
\label{sec:training-process-overview}
\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/overall-names.pdf}
  \caption{Full training process overview, illustrating the derivation of model variants. The diagram shows two parallel training paths starting from the pretrained \texttt{Qwen3-8B} baseline. The first path (top) shows the ICAE approach: the baseline initializes both encoder and decoder, then pretraining (PT) on SlimPajama-6B produces \texttt{ICAE-PT}, followed by fine-tuning (FT) on agentic trajectories to yield \texttt{ICAE-PT+FT}. Only encoder LoRA weights are trained while the decoder remains frozen. The second path (bottom) shows direct LoRA fine-tuning of the baseline on agentic trajectories, producing \texttt{Baseline+FT}. Thus, we receive all four main model variants for comparison.}
  \label{fig:training-process-overview}
\end{figure}

Figure~\ref{fig:training-process-overview} provides a comprehensive overview of the complete training and evaluation pipeline, illustrating how each of our model variants is derived.

The starting point for all variants is a standard, pretrained \texttt{Qwen3-8B} model~\cite{yang2025qwen3}, which we refer to as \texttt{Baseline}. This is a ready-to-use model, not one with random weights.
Note that the model in \texttt{Baseline} is already a pretrained model, but we stick with the naming of the process in \textcite{ge2023context} for consistency and name the two stages as Pretraining (PT) and Fine-Tuning (FT).

There are two parallel training paths. The first path is for our ICAE model. The \texttt{Baseline} model is used to initialize the ICAE encoder and decoder. In the Pretraining (PT) stage, we train the LoRA weights of the encoder on the SlimPajama-6B dataset~\cite{weber2024redpajama}, resulting in \texttt{ICAE-PT} model. This model is then fine-tuned on the agentic trajectories dataset, yielding the final \texttt{ICAE-PT+FT} model. Crucially, for both ICAE training stages, only the encoder's LoRA weights are updated, while the base Qwen3 model used as the decoder remains frozen.

The second path is for a comparative baseline. The original \texttt{Baseline} Qwen3 model is directly fine-tuned with LoRA on the agentic trajectories dataset. This produces the \texttt{Baseline+FT} model. This allows us to compare our two-stage ICAE approach against a standard parameter-efficient fine-tuning of a base language model on the target task.

In addition to the process overview in Figure~\ref{fig:training-process-overview}, Table~\ref{tab:model_variants_overview} provides a precise mapping of our four main model variant names to their respective encoder and decoder configurations. The encoder is responsible for compressing, while the decoder generates the text.
A dash (—) in the 'Encoder' column signifies that no compression module is used, and the decoder processes the uncompressed context directly. For a complete list of all experimental configurations and their results, please refer to Table~\ref{tab:qwen_icae_variants_absolute} in Chapter~\ref{cha:experiments}.

\begin{table}[h]
  \centering
  \small
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.05}
  \begin{tabular}{|l|ll|}
      \hline
      \textbf{Name in our paper} & \textbf{Encoder} & \textbf{Decoder} \\
      \hline
      \multicolumn{3}{|l|}{\hspace{1em}\textit{Baselines}} \\
      \hline
      \texttt{Baseline}                  & —                         & Qwen   \\
      \texttt{Baseline+FT}               & —                         & Qwen (LoRA-finetuned)   \\
      \hline
      \multicolumn{3}{|l|}{\hspace{1em}\textit{ICAE Compression}} \\
      \hline
      \texttt{ICAE-PT}                   & ICAE (LoRA-pretrained)         & Qwen           \\
      \texttt{ICAE-PT+FT}                & ICAE (LoRA-pretrained \& LoRA-finetuned)       & Qwen           \\
      \hline
  \end{tabular}
  \caption{
    This table maps the names of our four main model variants to their specific encoder and decoder configurations, as depicted in Figure~\ref{fig:training-process-overview}. 
    The encoder is responsible for compressing the context, while the decoder generates the text.
    A dash (—) in the "Encoder" column signifies that no compression module is used. Qwen stands for Qwen3-8B model. Pretraining and fine-tuning stages of the training process are described in Section~\ref{sec:pretraining} and Section~\ref{sec:fine-tuning}, respectively.
    A more comprehensive comparison including other variants is presented in Chapter~\ref{cha:experiments}.
  }
  \label{tab:model_variants_overview}
\end{table}
% ========================================
% SECTION 4.1: ICAE FOR AGENTIC CONTEXT MANAGEMENT
% ========================================
\section{ICAE for Agentic Context Management}

The In-Context Autoencoder (ICAE) \cite{ge2023context} consists of two modules: a trainable encoder (typically a LoRA-adapted LLM) and a fixed decoder (the base LLM itself).
The encoder processes a long context and generates a fixed number of learnable memory tokens.
This design turns a long, potentially unwieldy context into a compact representation that the decoder can efficiently consume.
The number of memory tokens controls the compression ratio, and their placement influences how the decoder accesses the stored information \cite{ge2023context}.

Figure~\ref{fig:icae} depicts the encoder–decoder split.
The encoder ingests the full context and produces memory tokens.
The frozen decoder then receives these tokens and a prompt to generate a continuation.
During pretraining, the encoder is optimized to enable the decoder to reconstruct the original text.
During fine-tuning, the objective shifts to solving a downstream task (e.g., generating a tool call) using the compressed representation.
Parameter-efficient methods, such as LoRA \cite{hu2022lora}, are used to adapt the encoder while preserving the base model's capabilities intact.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/icae.pdf}
  \caption{The architecture of the In-Context Autoencoder (ICAE).
  The encoder, a LoRA-adapted LLM, compresses a long context into a few memory slots.
  The decoder, a frozen base LLM, uses these memory slots (instead of the original context) and a special token (ST) to perform one of two tasks: autoencoding (i.e., text reconstruction) or language modelling (i.e., text continuation).
  Note that the special token (ST) is just used to signal the type of task to the decoder.
  }
  \label{fig:icae}
\end{figure}
Our main contribution is the adaptation and application of this framework to an agentic setting.
In this scenario, an agent interacts with an environment over multiple turns, generating a trajectory of actions and observations.
Our method utilizes ICAE to compress long observations, keeping the overall context manageable without losing critical information.

Beyond the basic encoder-decoder structure, the effectiveness of ICAE in our case depends critically on positional information.
Further enhancing this process, we incorporate insights on the strategic placement of compressed representations, as detailed by \textcite{zhao2024position}.
The effectiveness of context compression is highly dependent on the layout of position identifiers (IDs), particularly in models employing relative position embeddings like RoPE, which we discussed in Chapter~\ref{cha:background}.
These mechanisms create a local inductive bias where tokens with smaller positional distances exhibit stronger attention affinity.
Consequently, the placement of memory tokens in the position ID space dictates their accessibility to the decoder.

Standard methods often place compressed tokens at the beginning of the context, creating a large positional gap between the compressed history and the current prompt.
The work by \textcite{zhao2024position} demonstrates that minimizing this distance significantly improves performance.
To achieve this, we adopt their strategy of assigning position IDs to the memory tokens that are contiguous with those of the prompt.
This effectively "teleports" the compressed context to be adjacent to the current generation step, minimizing the relative distance \(j-i\) used in the attention calculation.
As a result, the decoder can attend to the compressed information as if it were part of the immediate local context.
The adoption of this position ID manipulation strategy yielded significant improvements in our experiments, particularly during the pretraining phase (see Figure~\ref{fig:pretraining-metrics-combined}).

% ========================================

% ========================================
% SECTION 4.2: TRAINING METHODOLOGY FOR AGENTIC ICAE
% ========================================
\section{Training Methodology for Agentic ICAE}

The process for training ICAE in an agentic scenario is depicted in Figure~\ref{fig:icae-agent-training-overview}.
The training data consists of pre-recorded agent trajectories (see an example of a trajectory in Example~\ref{ex:trajectory-compression}), which are sequences of alternating actions and observations, generated by an expert agent (in our case, Claude 3.7 Sonnet, see Figure~\ref{fig:icae-agent-training-overview}).
The collection of these trajectories is described in Section~\ref{sec:agentic_trajectories_data}.

At the start of the interaction, an uncompressed user prompt is provided (e.g., system prompt and task description in Example~\ref{ex:trajectory-compression}).
Then, a trajectory unfolds as an agent takes an action (tool call/action, e.g., \texttt{bash: ls -la} in Example~\ref{ex:trajectory-compression}), which is sent to an environment, and receives an observation in return.
This observation becomes input for the next step.
During the trajectory, compression is applied to every long observation (e.g., observations 2 and 3 in Example~\ref{ex:trajectory-compression}), ensuring that the decoder model never processes lengthy raw text.
Instead, it operates on compact embedding representations.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/mega-1.pdf}
  \caption{Overview of the agentic trajectory generation process used for fine-tuning data.
  An expert agent (concretely, Claude 3.7 Sonnet) starts with an initial prompt and task description.
  It then enters a loop, generating a tool call and receiving an observation from the environment.
  These sequences are collected to form the ground-truth trajectories.
  Only long observations, marked with an asterisk (*), are selected for compression during the fine-tuning of the ICAE model.
  The data collection process is further detailed in Section~\ref{sec:agentic_trajectories_data}.}
  \label{fig:icae-agent-training-overview}
\end{figure}

Figure~\ref{fig:icae-agent-training-step} illustrates a single fine-tuning step.
The observation from the environment is passed to the ICAE encoder, which produces a compressed representation.
This representation, along with the prior conversation history, is fed to the frozen decoder to generate the next tool call.
The training loss is the cross-entropy between the generated tool call and the reference action from the expert trajectory.
This loss is backpropagated through both the decoder and encoder to update only the encoder's LoRA weights, while the base model weights remain frozen.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=\textwidth]{graphs/mega-2.pdf}
  \caption{A single fine-tuning step for the agentic ICAE.
  The most recent observation is passed to the encoder to generate memory tokens.
  These tokens, combined with the history of previous actions and compressed observations, are fed to the frozen decoder.
  The decoder predicts the next action, and the loss between the predicted action and the reference action from the expert trajectory is backpropagated to update only the encoder's LoRA weights.
  }
  \label{fig:icae-agent-training-step}
\end{figure}

\subsection{Pretraining (PT)}
\label{sec:pretraining}
The first stage follows the original ICAE formulation~\cite{ge2023context}, pretraining the encoder on a large, general-purpose text corpus.
Two self-supervised objectives are constructed from text sequences in equal proportion (with a 50\% probability for each):
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Autoencoding (AE)}, where ICAE restores the original input text from its memory slots. This task is signaled by a special \texttt{<AE>} token, as conceptually illustrated in Figure~\ref{fig:icae}.
    \item \textbf{Language Modeling (LM)}, which predicts the continuation of a context to improve generalization, without the use of any special tokens.
\end{enumerate}
During this stage, only the encoder's LoRA weights are trained.
The goal is to teach the encoder to produce embeddings from which the frozen decoder can effectively reconstruct or continue text.

\subsection{Fine-Tuning (FT)}
\label{sec:fine-tuning}
After pretraining, the ICAE encoder is fine-tuned on a dataset of agentic trajectories (for more details, see Section~\ref{sec:agentic_trajectories_data}).
It should be noted that only the encoder's LoRA weights are updated.
The objective is to maximize the probability of generating the correct agent action (i.e., tool call) conditioned on the memory slots (for compressed observations) and the rest of the previous trajectory history.
Each turn in a trajectory is treated as a separate training sample, where the model predicts the next action given the trajectory history up to that point. The last observation is compressed and trained, while all the previous observations are saved during the previous turns and fed in as compressed embeddings.

During training, we optimize over single-step transitions.
At a timestep \(k\), the encoder compresses the observation \(o_{k-1}\).
The decoder then generates action \(a_k\) from the compressed history.
The loss from \(a_k\) is backpropagated to update the encoder's LoRA weights.
Importantly, backpropagation is performed only through a single turn — we do not backpropagate through the entire trajectory history as this would not be feasible due to the memory constraints.
Crucially, whenever an observation exceeds a predefined threshold (e.g., 256 tokens), the encoder compresses it into a fixed set of memory tokens.
This ensures the model never processes the full raw text of long observations, allowing it to handle arbitrarily long trajectories without exceeding context limits.

\subsection{Disabling "Thinking" for Tool Use}
Finally, a specific configuration choice for our agentic experiments involves the model's reasoning mode. The Qwen3 family includes a "thinking" feature designed for complex chain-of-thought reasoning. However, for our tool-use objectives, we explicitly turn off this feature to prioritize direct action generation and to simplify the context management pipeline. This decision is analyzed further in the ablation study in Appendix~\ref{sec:disabling_thinking}.
