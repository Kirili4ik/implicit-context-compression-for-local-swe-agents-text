% ========================================
% CHAPTER 3: RELATED WORK (WHAT WAS PREVIOUSLY DONE)
% ========================================
\chapter{Related Work}

\textbf{note: here i write "what was previously done". We write here "concrete: paper-what-is-the-difference".}


% ========================================
% SECTION 3.1: ARCHITECTURAL APPROACHES FOR LONG CONTEXT MODELING
% ========================================
\section{Architectural Approaches for Long Context Modeling}


% ========================================
% SECTION 3.2: SOFT PROMPTING, CONTEXT DISTILLATION, AND CONTINUOUS-THOUGHT REPRESENTATIONS
% ========================================
\section{Soft Prompting, Context Distillation, and Continuous-Thought Representations}

\subsection{CoCoNut +-}
Soft prompting is a method for conditioning large language models by learning continuous prompt vectors or prefix tokens rather than discrete text. 
These learned vectors are typically prepended to the model's input sequence and serve as a compact, trainable interface for adapting frozen decoders.
Classic methods in this category include prefix-tuning \cite{li_prefix_2021} and prompt tuning \cite{lester_prompt_2021}.
Both approaches involve optimizing a small set of continuous embeddings that steer the model toward desired behavior without full-scale model finetuning.
This parameter-efficient interface enables flexible adaptation and can be tuned for domain- or task-specific goals.
Because the prompt vectors are not constrained to map onto human-readable tokens, they can condense much more information than would be possible with standard textual prompts.

In practical, agentic settings the challenge of long or growing histories becomes acute (???).
%Models can distill the relevant information into a compact embedding or set of memory tokens.
%This reduces latency and memory overhead, as only condensed facts are passed forward during multi-step interactions.
%Examples of this pattern include ReAct-style prompting, which accumulates observations and actions over iterative reasoning steps \cite{yao_react_2022}, and Toolformer, which learns to use tools with self-supervised trajectories and requires summarizing complex action histories \cite{schick_toolformer_2023}.
%A scientific motivation for context distillation and soft prompting is that the resulting compression can be optimized directly for downstream utility, such as task success, rather than mere faithfulness to the original text.
%By learning what to keep and what to discard, these methods can mitigate biases and error propagation that affect explicit retrieval or summarization.

\textbf{CoConut} (Chain of Continuous Thought) \cite{coconut_placeholder,arxiv_2412_06769} generalizes the concept of soft prompting by moving away from natural language tokens altogether and enabling reasoning directly in latent, continuous spaces.
Instead of relying on explicit tokenization and sequence rewriting, CoConut leverages the model's own hidden states as a "continuous thought" vector.
After processing an input, the final hidden state (or a structured set of latent embeddings) is fed back as a contextual scaffold for further reasoning steps.
% Unlike prompt tuning, which fixes a learned vector prepended to all inputs, CoConut operates on an ever-evolving latent summary, tailored to the agentâ€™s state as the interaction progresses.
Experiments show that directly reasoning in the model's own latent space improves downstream performance for tasks with extended, multi-step dependencies, outperforming classic chain-of-thought prompting.
By discarding the constraints of discrete tokenization, CoConut demonstrates that agentic LLMs can reason, plan, and retain context in a fundamentally more expressive and compact way.



\section{Some attempts to do similar things like coconut or tobiases blogpost??}

\textbf{TODO: make a proper literature review..?}

Tobias' blogpost from openhands is not implicit, but explicit. so i have no knowledge of the same solutions using implicit compression?

Should write here about the other solutions for our problem/dataset? but what are those?