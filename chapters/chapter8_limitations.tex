% ========================================
% CHAPTER 6: LIMITATIONS AND FUTURE WORK (WHAT CAN WE NOT COVER)
% ========================================
\chapter{Limitations and Future Work}
\label{cha:limitations}

\section{Limitations of Fixed-Length Context Condensation}
\label{sec:limitations}

A core methodological limitation of the investigated approach is its reliance on a fixed number of memory tokens (e.g., 256) for context condensation. This design choice imposes a hardcoded, fixed compression ratio. For instance, a model configured with 256 memory tokens and a 4x compression ratio can only process a maximum of 1024 tokens of context at once. For longer inputs, such as an observation of 10,000 tokens, the condensation process would need to be applied iteratively in a loop. This would likely be slow and undermine the efficiency gains of the approach.

This fixed-length strategy is based on the assumption that all tokens in the context are equally important, which aligns with lossless autoencoding. However, this assumption becomes problematic at the high, lossy compression ratios required for significant context reduction. Experimental results confirm that performance attenuates or fails at high compression ratios (e.g., beyond 15x or 31x).


\section{Limitations of KV-caching}
\label{sec:kv_caching}
A notable limitation of our experimental setup is the exclusion of Key-Value (KV) caching, a standard optimization for autoregressive inference in Transformer models.
For methodological simplicity and to isolate the effects of context compression, in our experiments we recomputed the full attention state at each decoding step.

However, the ICAE framework is fully compatible with KV-caching.
The continuous embeddings produced by the encoder can be treated as a fixed prefix, and their corresponding key-value states can be pre-computed and cached.
Subsequent token generation would then reuse these cached states, significantly improving the absolute speed of the decoding process.
While KV-caching is essential for production deployment to achieve practical inference speeds, it was not necessary for our comparative evaluation.

\section{Constraints on Computational Resources}
\label{sec:comp_resources}

\subsection{Model Scale}
Due to computational and time constraints, our experiments were confined to the Qwen3-8B model. While evaluating on the full 500-issue SWE-bench Verified dataset provides sufficient statistical power for robust comparisons at this scale, an important direction for future research is to investigate ICAE's effectiveness on larger models like Qwen3-32B. It remains an open question whether larger models, with their increased capacity, can better leverage compressed representations or if they are more sensitive to information loss during compression.

\subsection{Full Fine-Tuning and the LoRA Bottleneck}
Our experiments with ICAE exclusively utilized LoRA for parameter-efficient fine-tuning, consistent with the original work.
However, our \texttt{baseline} experiments revealed a critical limitation: the LoRA-tuned baseline significantly underperformed the frozen base model in the agentic setting, whereas the fully fine-tuned model established the high-performance upper bound.
This "LoRA bottleneck" suggests that low-rank adaptation may be insufficient for learning the complex, multi-step reasoning required for autonomous software engineering, regardless of context compression.
Consequently, applying full fine-tuning to the ICAE encoder is not merely an optimization but likely a necessity for this domain.
Our open-sourced codebase facilitates this, and we identify it as the most important next step for future research.

\subsection{Disabled Reasoning}
For methodological simplicity, we disabled the Qwen3 decoder model's reasoning(i.e. "thinking") mechanism, which is designed to improve performance on complex reasoning tasks. Enabling this feature could prove highly beneficial, as it might allow the model to iteratively reason over the compressed knowledge stored in memory slots, potentially leading to better decision-making. Given that chain-of-thought reasoning has been shown to dramatically improve performance on various benchmarks, exploring the interaction between compressed context and explicit reasoning steps is a critical avenue for future research.


\section{Open Source Contributions and Reproducibility}
\label{sec:open_source}

We reimplemented the ICAE framework from scratch, as the original authors' code was outdated and difficult to adapt to our experimental needs.
Our implementation is modular and provides separate training pipelines for both pretraining (PT) and fine-tuning (FT).
We support pretraining on general text datasets and fine-tuning on question-answering, repo-qa tasks and agentic trajectories.

To advance the field of context compression for software engineering agents, we release our complete implementation, including pretrained models achieving 95\% reconstruction BLEU.
To ensure full reproducibility, we provide our complete implementation\footnote{\url{https://github.com/JetBrains-Research/icae}}, full Weights \& Biases experiment logs for pretraining\footnote{\url{https://wandb.ai/kirili4ik/icae-pretraining}} and fine-tuning\footnote{\url{https://wandb.ai/kirili4ik/icae-swebench-finetune}}, and model checkpoints\footnote{\url{https://huggingface.co/Kirili4ik/icae}}.
Our comprehensive release includes all training configurations, hyperparameters, and experiment logs, enabling future researchers to reproduce our results and build upon this work.
This open-source release provides the tools and transparency necessary for scientific progress in context management for LLMs.