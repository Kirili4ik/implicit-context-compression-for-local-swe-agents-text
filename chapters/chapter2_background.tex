% ========================================
% CHAPTER 2: BACKGROUND (FUNDAMENTALS)
% ========================================
\chapter{Background}


% ========================================
% SECTION 2.1: TRANSFORMER ARCHITECTURE AND POSITIONAL BIAS
% ========================================
\section{Transformer Architecture and Positional Bias}

Overview of the Transformer architecture and the role of position encodings (PEs) in injecting positional awareness and local inductive biases.
Discussion that position ID proximity often correlates with higher attention scores, establishing the importance of position layout in compression.

% \begin{definition}[Definitheit]
%   Hier definieren wir definitive Definitheit.
% \end{definition}
% 
% \begin{theorem}[vom X]
%   War wohl nix. Es gilt aber
%   \begin{align*}
%     \sum_{i=1}^{n} f_i(x) = \int \hat{f}(x) \dl{x}
%   \end{align*}
% \end{theorem}


% ========================================
% SECTION 2.2: PARAMETER-EFFICIENT LLM ADAPTATION (LORA)
% ========================================
\section{Parameter-Efficient LLM Adaptation (LoRA)}

Explanation of Low-Rank Adaptation (LoRA) as the chosen technique for parameter-efficiently adapting the LLM encoder during training.


% ========================================
% SECTION 2.3: AGENTIC ENVIRONMENT AND TOOL USE
% ========================================
\section{Agentic Environment and Tool Use}

Description of the necessity for LLM agents to process sequential action-observation data (trajectories) when interacting with a computer or environment.
Introduction to the tools provided to the assistant agent, such as bash, submit, and str\_replace\_editor (a custom editing tool).
