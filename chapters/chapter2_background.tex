% ========================================
% CHAPTER 2: BACKGROUND (FUNDAMENTALS)
% ========================================
\chapter{Background}
\textbf{note: here i write explanations of things. not too basic (not transformer), but at the same time not too specific for the thesis (not icae)}


% ========================================
% SECTION 2.1: TRANSFORMER ARCHITECTURE AND POSITIONAL BIAS
% ========================================
\section{Transformer Architecture and Positional Bias}

Self-attention mechanism is well-known and described in detail in many articles. Thus, we focus on how positional signals influence which tokens are likely to interact, and why the layout of position identifiers (position IDs) matters for context compression.

Transformers are permutation-invariant over their inputs unless augmented with positional information. In the original formulation, absolute position encodings \cite{vaswani_attention_2017} (either fixed sinusoidal or learned) are added to token embeddings so that attention has access to token order. 
Subsequent works replace addition with position-dependent biases or transformations that make attention explicitly sensitive to token distances:

\begin{itemize}
  \item Relative position representations add an index-dependent bias to the attention logits \cite{shaw_relative_2018}.
  \item Rotary position embeddings (RoPE) apply a rotation to queries and keys so that their inner product becomes a function of relative displacement \cite{su_roformer_2021}.
\end{itemize}

Formally, for queries \(Q\), keys \(K\), and values \(V\), attention often takes the form
\[
\operatorname{Attn}(Q,K,V) = \operatorname{softmax}\Big( \frac{QK^\top + B}{\sqrt{d_k}} \Big) V,\
\]
where \(B\) is a position-dependent bias. In relative schemes, \(B_{ij} = b(i-j)\). In RoPE, the similarity \(\langle Q_i, K_j\rangle\) implicitly depends on \(i-j\) via rotations applied to \(Q_i\) and \(K_j\).

These mechanisms create a local inductive bias: tokens that are closer in position tend to have higher prior attention affinity.
This has direct implications for compression with special tokens ("memory", or compressed tokens): where those tokens are placed in position-ID space controls which parts of the sequence they can most easily interact with. 
Empirically, assigning position IDs to minimize distance between compressed tokens and the tokens they must interface with (either source content or the downstream prompt) improves effectiveness \cite{zhao_position_2025}. 
\textbf{Maybe say we would talk about it later in the thesis?}

% \begin{definition}[Definitheit]
%   Hier definieren wir definitive Definitheit.
% \end{definition}
% 
% \begin{theorem}[vom X]
%   War wohl nix. Es gilt aber
%   \begin{align*}
%     \sum_{i=1}^{n} f_i(x) = \int \hat{f}(x) \dl{x}
%   \end{align*}
% \end{theorem}


% ========================================
% SECTION 2.2: PARAMETER-EFFICIENT LLM FINE-TUNING
% ========================================
\section{Parameter-Efficient LLM Fine-Tuning}

There are many techniques to efficiently fine-tune LLMs, but we focus on Low-Rank Adaptation (LoRA) \cite{hu2021lora}.
LoRA freezes the pre-trained weight matrices and introduces trainable low-rank updates, yielding substantial parameter savings while preserving the base model’s knowledge \cite{hu2021lora}. 
Consider a linear projection with base weight \(W_0 \in \mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}\). 
LoRA parameterizes an additive update
\[
\Delta W = B A,\quad A \in \mathbb{R}^{r \times d_{\text{in}}},\; B \in \mathbb{R}^{d_{\text{out}} \times r},\; r \ll \min(d_{\text{in}}, d_{\text{out}}),
\]
so that the adapted layer computes
\[
h = (W_0 + \alpha / r \cdot B A)\, x = W_0 x + \alpha / r \cdot B (A x),
\]
with a scalar scaling \(\alpha\) controlling the update magnitude. Only \(A\) and \(B\) are trained; \(W_0\) remains frozen. The trainable parameter count becomes \(r(d_{\text{in}} + d_{\text{out}})\), dramatically less than \(d_{\text{in}}\, d_{\text{out}}\) for typical ranks (e.g., tens to a few hundreds).

In Transformer blocks, LoRA adapters are commonly inserted in attention projections (query and value projections, see \cite{hu2021lora}) and optionally in output or feed-forward projections, trading off capacity and efficiency. 
The benefits include: 
\begin{itemize}
    \item parameter efficiency and reduced activation memory
    \item modularity—multiple task-specific adapters can be swapped atop a single base model
    \item faster fine-tuning
\end{itemize}
Practical considerations include choosing the rank \(r\), the scaling \(\alpha\), as well as target layers to balance adaptation capacity and generalization \cite{hu2021lora}.


% ========================================
% SECTION 2.3: AGENTIC SETUP AND TOOL USE
% ========================================
\section{Agentic Setup and Tool Use}

We use "agentic" to refer to autonomous decision-making loops in which an LLM plans, invokes tools, and incorporates observations to pursue goals. 
At time \(t\), the agent conditions on a history \(H_t = [(a_1,o_1),\dots,(a_{t-1},o_{t-1})]\) and a task-specific system prompt \(s\) to select an action \(a_t\). 
The environment (or tool) returns an observation \(o_t\), which is appended to the history. This perception–action loop continues until termination. Concretely:
\begin{enumerate}
  \item Prompt assembly: system instructions + task + concise guidelines + recent trajectory summary.
  \item Policy step: the LLM proposes an action (often with brief rationale) and a tool to invoke.
  \item Tool execution: the specified tool runs non-interactively with provided arguments.
  \item Observation: tool output (stdout/stderr, structured JSON, file diffs, or retrieval snippets) is captured.
  \item Memory update: \((a_t, o_t)\) is logged; optional compression/summarization reduces footprint.
  \item Termination or next step: the agent either returns a final answer or continues planning.
\end{enumerate}


A prominent benchmark for evaluating such agentic capabilities is the Berkeley Function Calling Leaderboard (BFCL) \cite{patil2025bfcl}. 
BFCL provides a standardized suite of tasks to measure an LLM's proficiency in translating natural language requests into precise, executable tool calls. 
The tasks range from simple, single-function invocations to complex scenarios requiring multi-step reasoning and tool chaining. 
This benchmark exemplifies the practical challenges in agentic systems, where models must correctly interpret user intent and interact with external APIs or codebases \cite{patil2025bfcl}.


\textbf{Agentic toolcall examples (from \cite{patil2025bfcl}):}
\begin{itemize}
  \item Vehicle status lookup: \texttt{vehicle.getStatus(vin="WVWZZZ...")} — returns battery level, tire pressure, and last seen location.
  \item Driving route plan: \texttt{maps.route(origin="Munich", dest="Berlin", mode="driving")} — computes ETA and step-by-step directions.
  \item Hotel search: \texttt{booking.search(city="Prague", dates="2025-11-03..05", guests=2)} — lists options with price and rating.
  \item Weather check: \texttt{weather.current(city="Warsaw")} — returns temperature, precipitation, and alerts.
\end{itemize}

\textbf{Code-oriented toolcall examples}
\begin{itemize}
  \item Code/bash execution: \texttt{execute(command="pytest -q")} or \texttt{execute(command="ls -la")} — runs unit tests using pytest or lists files with details.
  \item Symbol search: \texttt{find(name="parseUser")} — finds the definition and usages of a function in the codebase.
  \item String replacement in code: \texttt{str\_replace(file\_path="app.py", search="foo", replace="bar")} — replaces all occurrences of "foo" with "bar" in app.py.
\end{itemize}

%%% ### here describe React + cite \cite{yao_react_2022}